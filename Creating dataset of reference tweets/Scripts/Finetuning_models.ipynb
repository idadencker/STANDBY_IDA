{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a2b124-9d82-4734-8e8b-b8b118863e01",
   "metadata": {},
   "source": [
    "# Finetuning a model on labeled counterspeech data (CONAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb71774-f4ab-4aa7-a483-921870237682",
   "metadata": {},
   "source": [
    "CONAN (COunter NArratives through Nichesourcing): A dataset containing counterspeech responses to hate speech. Data is French, italian and English\n",
    "\n",
    "https://github.com/marcoguerini/CONAN\n",
    "\n",
    "https://aclanthology.org/P19-1271.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2819d42-0567-44f9-be49-4a4be22b282e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.44.0)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.20.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets pandas scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea90e5a-5061-46e0-b386-96d889b7a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6803\n",
      "856\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conan = pd.read_csv('/Users/idahelenedencker/Desktop/CONAN.csv')\n",
    "conan\n",
    "\n",
    "# Count how many unqie instances of each\n",
    "print(conan['counterSpeech'].nunique())\n",
    "print(conan['hateSpeech'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7e98a9-2be5-49f9-bf33-eeeef58d415b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    6803\n",
       "0     856\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine hate speech and counter speech into a single DataFrame\n",
    "data = {\n",
    "    'text': conan['hateSpeech'].tolist() + conan['counterSpeech'].tolist(),\n",
    "    'label': [0] * len(conan['hateSpeech']) + [1] * len(conan['counterSpeech'])}\n",
    "\n",
    "# Make into df\n",
    "df_prepared = pd.DataFrame(data)\n",
    "\n",
    "# Drop duplicates\n",
    "df_prepared.drop_duplicates(inplace=True)\n",
    "\n",
    "# Count each label\n",
    "df_prepared.value_counts(df_prepared['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6d857a-d08c-415f-b4cd-888a47ea322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to a recent OFSTED report, a school ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Birmingham there is a school where girls an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A school in Birningham is still separating gir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The police and politicians are covering up the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Muslims grooming gangs are protected by the go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29967</th>\n",
       "      <td>Italy unfortunately is full of unveiled women ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29968</th>\n",
       "      <td>Nessuna donna accetta di essere picchiata, non...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29971</th>\n",
       "      <td>No woman agrees to be beaten, be unable to bre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29972</th>\n",
       "      <td>Leggere commenti di donne capaci di scrivere t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29975</th>\n",
       "      <td>Reading comments of women who can write such n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7659 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      According to a recent OFSTED report, a school ...      0\n",
       "1      In Birmingham there is a school where girls an...      0\n",
       "2      A school in Birningham is still separating gir...      0\n",
       "12     The police and politicians are covering up the...      0\n",
       "13     Muslims grooming gangs are protected by the go...      0\n",
       "...                                                  ...    ...\n",
       "29967  Italy unfortunately is full of unveiled women ...      1\n",
       "29968  Nessuna donna accetta di essere picchiata, non...      1\n",
       "29971  No woman agrees to be beaten, be unable to bre...      1\n",
       "29972  Leggere commenti di donne capaci di scrivere t...      1\n",
       "29975  Reading comments of women who can write such n...      1\n",
       "\n",
       "[7659 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebf0a22-36ab-4e80-a635-9d9ce4357432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1293' max='1293' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1293/1293 28:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.168110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.129995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.158067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.15806695818901062, 'eval_runtime': 14.2365, 'eval_samples_per_second': 53.805, 'eval_steps_per_second': 3.372, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuning/tokenizer/tokenizer_config.json',\n",
       " './finetuning/tokenizer/special_tokens_map.json',\n",
       " './finetuning/tokenizer/vocab.txt',\n",
       " './finetuning/tokenizer/added_tokens.json',\n",
       " './finetuning/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = df_prepared\n",
    "\n",
    "# Split into training and validation sets (90% train, 10% validation)\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Prepare the data for Hugging Face\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "val_texts = val_df['text'].tolist()\n",
    "val_labels = val_df['label'].tolist()\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with other models like \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification\n",
    "\n",
    "# Tokenize the text data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class CounterspeechDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = CounterspeechDataset(train_encodings, train_labels)\n",
    "val_dataset = CounterspeechDataset(val_encodings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define the training arguments with modified paths\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./finetuning/results',          # output directory for model checkpoints\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./finetuning/logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",           # Save the model after each epoch\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer in the 'finetuning' directory\n",
    "model.save_pretrained(\"./finetuning/model\")\n",
    "tokenizer.save_pretrained(\"./finetuning/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d06e3aa-fae7-4db6-a9a6-98bf1c312048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a sentence\n",
    "\n",
    "input_text = \"i like horses\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Move inputs to CPU\n",
    "inputs = {key: val.cpu() for key, val in inputs.items()}\n",
    "\n",
    "# Move the model to CPU\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834e5223-dc84-4f18-81da-998911d21dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the 20k pairs data\n",
    "\n",
    "#load in the translated data\n",
    "#Load in\n",
    "dtype_dict_all = {\n",
    "    'conversation_id': 'object',\n",
    "    'id': 'object',\n",
    "    'author_id': 'object',\n",
    "    'referenced_tweets_id': 'object',\n",
    "    'in_reply_to_user_id': 'object',\n",
    "    'PNR': 'object'\n",
    "}\n",
    "\n",
    "#pairs = pd.read_csv('/Users/idahelenedencker/Desktop/w_translated_full.csv', dtype=dtype_dict_all)\n",
    "pairs = pd.read_csv('/Users/idahelenedencker/Desktop/STANDBY_Ida/Creating dataset of reference tweets/w_translated_small.csv', dtype=dtype_dict_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6874bd2e-63b7-4dc1-bdbc-3148c98ab38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/125 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_1127/3224080062.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [01:18<00:00,  1.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>replied_to_reply_count</th>\n",
       "      <th>referenced_tweets_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>PNR</th>\n",
       "      <th>...</th>\n",
       "      <th>fasttext_cos_sim_prosocial_sentence</th>\n",
       "      <th>tweeter_username</th>\n",
       "      <th>tweeter_name</th>\n",
       "      <th>pair_num</th>\n",
       "      <th>type</th>\n",
       "      <th>like_n</th>\n",
       "      <th>retweet_n</th>\n",
       "      <th>quote_n</th>\n",
       "      <th>translated</th>\n",
       "      <th>finetune_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1036583587242549248</td>\n",
       "      <td>da</td>\n",
       "      <td>2018-09-03 21:04:41</td>\n",
       "      <td>1036721666628444160</td>\n",
       "      <td>@frkomo Jeg siger det vel strengt taget bare t...</td>\n",
       "      <td>1666088336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1036721302692917250</td>\n",
       "      <td>148061237</td>\n",
       "      <td>1311570613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>reply</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@frkomo I guess I'm saying it strictly just to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1036583587242549248</td>\n",
       "      <td>da</td>\n",
       "      <td>2018-09-03 21:03:14</td>\n",
       "      <td>1036721302692917250</td>\n",
       "      <td>@MonbergSF Sig det til spillerforeningen, som ...</td>\n",
       "      <td>148061237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1666088336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frkomo</td>\n",
       "      <td>Sarah Agerklint</td>\n",
       "      <td>1</td>\n",
       "      <td>tweet</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@MonbergSF Tell it to the gaming association, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>899548260863488002</td>\n",
       "      <td>da</td>\n",
       "      <td>2017-08-21 12:10:42</td>\n",
       "      <td>899604671375052801</td>\n",
       "      <td>@PeterHuggler Had alt det, du vil. Men du skal...</td>\n",
       "      <td>547416021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>899603561323081729</td>\n",
       "      <td>3301029597</td>\n",
       "      <td>1405772015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>reply</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@PeterHuggler Had everything you want. But don...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>899548260863488002</td>\n",
       "      <td>da</td>\n",
       "      <td>2017-08-21 12:06:17</td>\n",
       "      <td>899603561323081729</td>\n",
       "      <td>@brianweichardt Jeg hader den her slags: Du g√•...</td>\n",
       "      <td>3301029597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>547416021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PeterHuggler</td>\n",
       "      <td>Peter Huggler</td>\n",
       "      <td>2</td>\n",
       "      <td>tweet</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@brianweichardt I hate this kind of thing: You...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1345496479583113217</td>\n",
       "      <td>da</td>\n",
       "      <td>2021-01-03 12:38:16</td>\n",
       "      <td>1345711074407014400</td>\n",
       "      <td>@nielscallesoe @Heunicke Din f√∏rste indvending...</td>\n",
       "      <td>87923613</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1345524516311748608</td>\n",
       "      <td>23341699</td>\n",
       "      <td>0908801199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>reply</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@nielsallesoe @Heunicke Your first objection m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1285495153202024448</td>\n",
       "      <td>da</td>\n",
       "      <td>2020-07-21 10:22:06</td>\n",
       "      <td>1285520419529863168</td>\n",
       "      <td>@SimonStoerup @perlysholt Hm. Man ville som ud...</td>\n",
       "      <td>27626050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383396359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nielsfez</td>\n",
       "      <td>Niels Pedersen</td>\n",
       "      <td>998</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@SimonStoerup @perlysholt Hm. You would be abl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1174395406719102976</td>\n",
       "      <td>da</td>\n",
       "      <td>2019-09-18 19:10:18</td>\n",
       "      <td>1174400270710820870</td>\n",
       "      <td>@R4nd4hl @khoenge Sjovt du synes netop H√∏nge b...</td>\n",
       "      <td>861057936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1174395406719102976</td>\n",
       "      <td>72823792</td>\n",
       "      <td>1508892043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999</td>\n",
       "      <td>reply</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@R4nd4hl @khoenge Funny you think just H√∏nge s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1174395406719102976</td>\n",
       "      <td>da</td>\n",
       "      <td>2019-09-18 18:50:58</td>\n",
       "      <td>1174395406719102976</td>\n",
       "      <td>Detektor har unders√∏gt det: @khoenge talte usa...</td>\n",
       "      <td>72823792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R4nd4hl</td>\n",
       "      <td>Randahl Fink</td>\n",
       "      <td>999</td>\n",
       "      <td>tweet</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Detector has examined it: @khoenge spoke untru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1481298462238982144</td>\n",
       "      <td>da</td>\n",
       "      <td>2022-01-12 19:25:54</td>\n",
       "      <td>1481346717513662464</td>\n",
       "      <td>@ReneAndersenDK Forskellen er, at ham her ogs√•...</td>\n",
       "      <td>805874425988087811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1481345545721495554</td>\n",
       "      <td>4776986009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>reply</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@ReneAndersenDK The difference is that this gu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1481298462238982144</td>\n",
       "      <td>da</td>\n",
       "      <td>2022-01-12 19:21:14</td>\n",
       "      <td>1481345545721495554</td>\n",
       "      <td>@baretraet S√•dan et m√•l har vi sgu da alle sam...</td>\n",
       "      <td>4776986009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>805874425988087811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theotherguy_78</td>\n",
       "      <td>Ren√©</td>\n",
       "      <td>1000</td>\n",
       "      <td>tweet</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@baretraet Such a goal have we all made for tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          conversation_id lang           created_at                   id  \\\n",
       "0     1036583587242549248   da  2018-09-03 21:04:41  1036721666628444160   \n",
       "1     1036583587242549248   da  2018-09-03 21:03:14  1036721302692917250   \n",
       "2      899548260863488002   da  2017-08-21 12:10:42   899604671375052801   \n",
       "3      899548260863488002   da  2017-08-21 12:06:17   899603561323081729   \n",
       "4     1345496479583113217   da  2021-01-03 12:38:16  1345711074407014400   \n",
       "...                   ...  ...                  ...                  ...   \n",
       "1995  1285495153202024448   da  2020-07-21 10:22:06  1285520419529863168   \n",
       "1996  1174395406719102976   da  2019-09-18 19:10:18  1174400270710820870   \n",
       "1997  1174395406719102976   da  2019-09-18 18:50:58  1174395406719102976   \n",
       "1998  1481298462238982144   da  2022-01-12 19:25:54  1481346717513662464   \n",
       "1999  1481298462238982144   da  2022-01-12 19:21:14  1481345545721495554   \n",
       "\n",
       "                                                   text           author_id  \\\n",
       "0     @frkomo Jeg siger det vel strengt taget bare t...          1666088336   \n",
       "1     @MonbergSF Sig det til spillerforeningen, som ...           148061237   \n",
       "2     @PeterHuggler Had alt det, du vil. Men du skal...           547416021   \n",
       "3     @brianweichardt Jeg hader den her slags: Du g√•...          3301029597   \n",
       "4     @nielscallesoe @Heunicke Din f√∏rste indvending...            87923613   \n",
       "...                                                 ...                 ...   \n",
       "1995  @SimonStoerup @perlysholt Hm. Man ville som ud...            27626050   \n",
       "1996  @R4nd4hl @khoenge Sjovt du synes netop H√∏nge b...           861057936   \n",
       "1997  Detektor har unders√∏gt det: @khoenge talte usa...            72823792   \n",
       "1998  @ReneAndersenDK Forskellen er, at ham her ogs√•...  805874425988087811   \n",
       "1999  @baretraet S√•dan et m√•l har vi sgu da alle sam...          4776986009   \n",
       "\n",
       "      replied_to_reply_count referenced_tweets_id in_reply_to_user_id  \\\n",
       "0                        1.0  1036721302692917250           148061237   \n",
       "1                        NaN                  NaN          1666088336   \n",
       "2                        1.0   899603561323081729          3301029597   \n",
       "3                        NaN                  NaN           547416021   \n",
       "4                        1.0  1345524516311748608            23341699   \n",
       "...                      ...                  ...                 ...   \n",
       "1995                     NaN                  NaN           383396359   \n",
       "1996                     0.0  1174395406719102976            72823792   \n",
       "1997                     NaN                  NaN                 NaN   \n",
       "1998                     1.0  1481345545721495554          4776986009   \n",
       "1999                     NaN                  NaN  805874425988087811   \n",
       "\n",
       "             PNR  ... fasttext_cos_sim_prosocial_sentence  tweeter_username  \\\n",
       "0     1311570613  ...                            0.959726               NaN   \n",
       "1            NaN  ...                                 NaN            frkomo   \n",
       "2     1405772015  ...                            0.958750               NaN   \n",
       "3            NaN  ...                                 NaN      PeterHuggler   \n",
       "4     0908801199  ...                            0.957824               NaN   \n",
       "...          ...  ...                                 ...               ...   \n",
       "1995         NaN  ...                                 NaN          nielsfez   \n",
       "1996  1508892043  ...                            0.932785               NaN   \n",
       "1997         NaN  ...                                 NaN           R4nd4hl   \n",
       "1998         NaN  ...                            0.932775               NaN   \n",
       "1999         NaN  ...                                 NaN    theotherguy_78   \n",
       "\n",
       "         tweeter_name  pair_num   type like_n  retweet_n  quote_n  \\\n",
       "0                 NaN         1  reply      1          0        0   \n",
       "1     Sarah Agerklint         1  tweet      1          0        0   \n",
       "2                 NaN         2  reply      0          0        0   \n",
       "3       Peter Huggler         2  tweet      1          0        0   \n",
       "4                 NaN         3  reply      1          0        0   \n",
       "...               ...       ...    ...    ...        ...      ...   \n",
       "1995   Niels Pedersen       998  tweet      2          0        0   \n",
       "1996              NaN       999  reply      2          0        0   \n",
       "1997     Randahl Fink       999  tweet     20          3        0   \n",
       "1998              NaN      1000  reply      2          0        0   \n",
       "1999             Ren√©      1000  tweet      3          0        0   \n",
       "\n",
       "                                             translated finetune_predictions  \n",
       "0     @frkomo I guess I'm saying it strictly just to...                    1  \n",
       "1     @MonbergSF Tell it to the gaming association, ...                    1  \n",
       "2     @PeterHuggler Had everything you want. But don...                    1  \n",
       "3     @brianweichardt I hate this kind of thing: You...                    1  \n",
       "4     @nielsallesoe @Heunicke Your first objection m...                    1  \n",
       "...                                                 ...                  ...  \n",
       "1995  @SimonStoerup @perlysholt Hm. You would be abl...                    1  \n",
       "1996  @R4nd4hl @khoenge Funny you think just H√∏nge s...                    1  \n",
       "1997  Detector has examined it: @khoenge spoke untru...                    1  \n",
       "1998  @ReneAndersenDK The difference is that this gu...                    1  \n",
       "1999  @baretraet Such a goal have we all made for tr...                    1  \n",
       "\n",
       "[2000 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_name = \"./finetuning/model\"\n",
    "tokenizer_name = \"./finetuning/tokenizer\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "\n",
    "# Tokenize the new data\n",
    "encodings = tokenizer(pairs['translated'].tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#create a data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "new_dataset = CustomDataset(encodings)\n",
    "\n",
    "\n",
    "new_loader = DataLoader(new_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "#make predictions and evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(new_loader):\n",
    "        inputs = {key: val.to(model.device) for key, val in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "pairs['finetune_predictions'] = predictions\n",
    "\n",
    "pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ecf5bb-9fea-4a11-b732-236364d97ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune_predictions\n",
      "1    1988\n",
      "0      12\n",
      "Name: count, dtype: int64\n",
      "Marked as counterspeech:\n",
      "[\"@frkomo I guess I'm saying it strictly just to those who follow me;) But if you follow the subsequent discussion, then I'm just not impressed by the approach the players have:)\", '@MonbergSF Tell it to the gaming association, which is just fighting for DBU to have employer responsibility. Both with the gentlemen and the ladies', \"@PeterHuggler Had everything you want. But don't judge what I think is a natural reaction. I'm not covering that case at all.\", '@brianweichardt I hate this kind of thing: You go in and make yourself a judge, based on a photo. Let the court do its job, and seek only to communicate the case.', '@nielsallesoe @Heunicke Your first objection may be correct. It must be assessed. The second I do not understand. What other vaccines do we offer off-label?', '@stinuslindgreen @Heunicke In my optics, it is not a relevant concern. Several reasons. Main: First, because we do not have scale to make a difference. IF it gives increased selection in that direction, it happens no matter what we do. Second, we do not neglect other vaccines on that basis.', \"@RasmusMalver @radical I'm sorry. I want to understand your concern - that's why I answer you - but unfortunately I have a little difficulty following your reasoning. It sounds like we're talking past each other. Maybe it's Twitter? I don't know, but I always want to have a serious dialogue.\", \"Until an hour ago, @stinuslindgreen was on my top 5 over members of parliament. He is new in politics, but has really learned to bullshit. @Radio claimed that there was court control with access to private homes. There isn't. See how he tries to change direction. #dkpol https://t.co/37odfP9Y3O\", \"@larskohler @BEsbensen @PiaOlsen A little, but I would really like to understand your criticism!:) We don't hit broad enough do you write how?\", \"@ThomasMonbergSF I think we're running in a circle!... Have a good day and a go Pride!! ‚Ä¢ @BEsbensen @PiaOlsen\", '@gastronautdk @peterbrothersen We probably just perceive the debate differently. I just think you underline my point, even though that is hardly your intention', '@peterbrothersen Exactly... So thin post from @AnneFrostJepsen... \"Don\\'t beat the politicians even if they beat on a daily basis\" Adult communication requires a desire from the politicians to act adultly and we still need to see', \"@quitte74 @BrondbyIF Should I bring yours when I'm down there? I respect you disagree with me. But not your tone. It's unnecessary.\", '@JanniMT @BrondbyIF Please turn yourself down and pick up your suck, please.', '@HellaSchulin @dksvin @KopenhagenFurDK @disekammeren @MfVMin @MogensJensenS Well, I understand your opinions in the area qua your attachment. And that is absolutely fair enough. But democratically adopted laws should not be followed if we \"just feel\" something else? Is d the kind of politician you want to be if you get a mandate? (Both written with respect)', '@HenrikDyhHansen @dksvin @KopenhagenFurDK @spisekammeren @MFVMin @MogensJensenS Hmm, now our views on the justification of the industry are different. For me, this corresponds to compensation for running a slave farm. My view of ethics is not yet supported by the current system, I am well aware of that, but it is difficult for me to understand the attitudes towards animals.', \"@adamwolfregion @SigneLoentoft I didn't know that, what I'm saying is just that quite important topic for that type of twitter dispute. Isn't the direct way to more coherence/cooperation either. And the answer to your question we've long since given?\", \"@KWEkristian Ah, that's a rather thin answer when you have started the debate yourself - as you know, we have a board meeting, so I don't think there will be much regional participation in the first part of your meeting. @SigneLoentoft\", \"@Knud_brix @mortenskaerbaek Serious journalism, agree. The claim is just crazy, as it is a claim. Among too many others. Well, the infantile behavior is challenged. But let's bear it out. Have read that she likes Br√∏ndby.\", 'When @mortenskaerbaek is in top shape, it is far from the sock holders. There should just be sound at this interview https://t.co/yHk4vB74m9']\n",
      "Not marked as counterspeech:\n",
      "['@MonbergSF @CasparDyb Some of them cheered over ESL and they will probably also prefer an English league with 10-12 teams 2/2', 'There was a man yelling at me today at Kultursnob.', 'Countdown started for #NordicNoir tomorrow at Kurhotel Skodsborg @Skodsborg1898 with #KarinSlaughter and @sarablaedel, delights me BIG TIME!', '@PruPruPruh Fat ‚Ä¢ Karla hated hay ‚Ä¢', '@mbulskov @Wirlander @Cihat_Bardak @a_esbech @DanskDf1995 Mirror Bulskov', '@MikkelVenneberg @kasperlarsen11 @FidusBamsensFar K√¶ft it could have been peasant robbed of you hired Glen!', '@NazilaKivi Bags on seats people are the worst people', '@MortenBHojgaard @DanielLerke @Baumspieler @obvoter @bjerghest @SebStanbury @mpbdavidsen Enig', '@NazilaKivi If this is the level I chill out of here ‚Ä¢', \"@baretraet @PabloBresciani @JanEliazzen I don't have time for your childish nonsense!\", '@ZofiaLPetersen looked drop out', \"@NazilaKivi @PEDUARD And you think that because Islam is inherent we should ignore the burning of the Danish flag. They threaten and throw stones at homosexuals? They think they are in their right to create conflict because someone celebrates a holiday they don't themselves do. Should we then accept the stoning of women?\"]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the results\n",
    "\n",
    "# How many of each class\n",
    "print(pairs.value_counts(pairs['finetune_predictions']))\n",
    "\n",
    "#print\n",
    "finetune_yes= pairs[pairs['finetune_predictions'] == 1]\n",
    "finetune_no= pairs[pairs['finetune_predictions'] == 0]\n",
    "\n",
    "\n",
    "text_to_print = finetune_yes['translated'].head(20).tolist()\n",
    "print('Marked as counterspeech:')\n",
    "print(text_to_print) \n",
    "\n",
    "text_to_print = finetune_no['translated'].head(20).tolist()\n",
    "print('Not marked as counterspeech:')\n",
    "print(text_to_print) \n",
    "\n",
    "\n",
    "# Marks almost all as counterspeech which is not good, it's quite clear that what the model has used as label 0's (hatefull speech) in the training process contain hateful comments on islam, and hence what is labeled as 0's here contain mainly hateful/harsh language and/or islamic aspects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86869484-ea78-465b-a79b-a78088f859bd",
   "metadata": {},
   "source": [
    "# (maybe) Finetuning the bestperforming huggingface counterspeech classifier model on a labeled danish dataset (i can label some?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a43e36-99ef-4853-b53c-a816c04c0e1a",
   "metadata": {},
   "source": [
    "Dont know if this is problamatic, since the model will be finetuned on (a sample of) the same labeled dataset that i will afterwards apply the finetuned model to? \n",
    "\n",
    "An idea could be to remove the labeled sample from the test dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
