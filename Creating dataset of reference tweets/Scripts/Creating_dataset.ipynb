{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19738d37-1769-465f-a414-eeea3a606059",
   "metadata": {},
   "source": [
    "# Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4435c95d-c8f6-4088-b9dd-a395192b19e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (6.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#load pacakges \n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install torch \n",
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2b258-4de2-4a97-a563-a58e2e166a87",
   "metadata": {},
   "source": [
    "# Loading, cleaning and filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7a94d8-d507-463f-b6a1-3e533ad8ffb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conversation_id                  object\n",
       "lang                             object\n",
       "created_at                       object\n",
       "id                               object\n",
       "text                             object\n",
       "author_id                        object\n",
       "public_metrics.retweet_count      int64\n",
       "public_metrics.reply_count        int64\n",
       "public_metrics.like_count         int64\n",
       "public_metrics.quote_count        int64\n",
       "referenced_tweets_id             object\n",
       "referenced_tweets_type           object\n",
       "in_reply_to_user_id              object\n",
       "__index_level_0__               float64\n",
       "PNR                              object\n",
       "surveyXact_externke              object\n",
       "non_unique_twitter_author_id    float64\n",
       "started_survey                  float64\n",
       "cleaned_text                     object\n",
       "rec-nition                        int64\n",
       "attack                            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "# Define the dtype dictionary\n",
    "dtype_dict_all = {\n",
    "    'conversation_id': 'object',\n",
    "    'lang': 'object',\n",
    "    'created_at': 'object',\n",
    "    'id': 'object',\n",
    "    'text': 'object',\n",
    "    'author_id': 'object',\n",
    "    'referenced_tweets_id': 'object',\n",
    "    'referenced_tweets_type': 'object',\n",
    "    'in_reply_to_user_id': 'object',\n",
    "    'PNR': 'object',\n",
    "    'surveyXact_externke': 'object',\n",
    "    'cleaned_text': 'object'\n",
    "}\n",
    "\n",
    "# Load in the scored csv file with the dictionary\n",
    "scored_tweets= pd.read_csv('/Users/idahelenedencker/Desktop/STANDBY_Ida/Rec-nition and attack on twitter data/CSV files/twitter_w_rec_attack_final.csv',  dtype=dtype_dict_all)\n",
    "\n",
    "#check data types\n",
    "scored_tweets.dtypes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773fbcef-08b1-45ee-911b-767c0f5fb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert non_unique_twitter_author_id and started_survey into integer\n",
    "scored_tweets['non_unique_twitter_author_id'] = pd.to_numeric(scored_tweets['non_unique_twitter_author_id'], errors='coerce').astype('Int64')\n",
    "scored_tweets['started_survey'] = pd.to_numeric(scored_tweets['started_survey'], errors='coerce').astype('Int64')\n",
    "\n",
    "#check value counts\n",
    "scored_tweets['rec-nition'].value_counts(dropna=False) \n",
    "scored_tweets['attack'].value_counts(dropna=False) \n",
    "\n",
    "#filter to replied_to\n",
    "replies = scored_tweets[scored_tweets['referenced_tweets_type'] == 'replied_to']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beef1bc-ab82-4b0b-b51f-fb19a15657fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6d5b3-9331-4ecd-85c3-c5be08efadba",
   "metadata": {},
   "source": [
    "- conversation_id: the Post ID of the original Post that started the conversation. (if the post is not a Retweet, Quoted Tweet, or reply the conversation_id will be the same as id)\n",
    "- lang: language\n",
    "- created_at: when it was created (YYYY-MM-DDTHH:MM:SS.000Z, ISO8601 format giving: year, month, data at Time: hour:minute:second:milisecond in Coordinated Universal Time (UTC))\n",
    "- id: Each object within Twitter - a Tweet, Direct Message, User, List, and so on - has a unique ID\n",
    "- text: the post, starts with @ if it is a retweet or reply to\n",
    "- author_id: returns user object representing the Post's author\n",
    "- public_metrics.retweet_count: how many retweeted\n",
    "- public_metrics.reply_count: how many replied\n",
    "- public_metrics.like_count: how many liked\n",
    "- public_metrics.quote_count: how many quoted\n",
    "- referenced_tweets_id: returns post object that this Post is referencing (either as a Retweet, Quoted Tweet, or reply)\n",
    "- referenced_tweets_type: if it is a post (None), retweeted or replied_to etc.\n",
    "- in_reply_to_user_id: returns a user object representing the Post author this requested Post is a reply of\n",
    "- __index_level_0__: ??\n",
    "- PNR: personnummer (Eentydig identifikation af person), like cpr\n",
    "- surveyXact_externke: personification ID linked to the e-boks survey\n",
    "- non_unique_twitter_author_id (made by simon): ?? can take 0 or 1\n",
    "- started_survey: (made by simon): if they started the e-boks survey (doesnt mean finished it) can take 0 or 1\n",
    "\n",
    "\n",
    "\n",
    "Read more here: \n",
    "- https://developer.x.com/en/docs/twitter-ids (ID)\n",
    "- https://developer.x.com/en/docs/twitter-api/expansions (in_reply_to_user_id, author_id, referenced_tweets_id)\n",
    "- https://developer.x.com/en/docs/twitter-api/conversation-id (conversation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709e820-bdae-4317-8a68-3de84743591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at performance of algorithms\n",
    "\n",
    "#pull up text examples\n",
    "attack= replies[replies['attack'] == 1]\n",
    "print(attack['text'][:100])\n",
    "\n",
    "rec_nition= replies[replies['rec-nition'] == 1]\n",
    "print(rec_nition['text'][:100])\n",
    "\n",
    "# can print the whole text using\n",
    "text_to_print = rec_nition['text'].head(20).tolist()\n",
    "text_to_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498b404-8b21-4f06-b0d8-2e8592a5a3d2",
   "metadata": {},
   "source": [
    "# Limiting the df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75d2ac-62c9-4bd8-8c7b-2d8cbe2b20cb",
   "metadata": {},
   "source": [
    "## Random sampling (10k samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff0597-8a0b-482b-854a-ce540d0970d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sampling\n",
    "replies_random= replies.sample(10000)\n",
    "replies_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd221c6d-51e0-4f07-bd83-12006a5ff6b5",
   "metadata": {},
   "source": [
    "## Stratified sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca033f-d309-4a61-afbb-f8fb8e8d22d0",
   "metadata": {},
   "source": [
    "### Using attack/regnition split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b36199-947a-463d-93a5-38b5164c2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling using attack and rec-nition \n",
    "\n",
    "#idea: ¬º are scored 0-0 for attack-recnition; ¬º are scored 1-0 for attack-recnition, ¬º are scored 0-1 for attack-recnition, ¬º are scored 1-1 (possible that there are no/few examples that have both attack AND rec-nition labels) for attack-recnition \n",
    "\n",
    "# First: Check if there are any 1-1 cases\n",
    "condition = (replies['attack'] == 1) & (replies['rec-nition'] == 1)\n",
    "rows_with_condition = replies[condition]\n",
    "rows_with_condition #1156 cases (but not enough to be 25%)\n",
    "\n",
    "#new idea: 1000 of 1-1, 3000 1-0, 3000 0-1, 3000 0-0\n",
    "\n",
    "#make a new column 'A/R' with both scorings can take 0-0, 0-1, 1-1, or 1-0\n",
    "replies['A/R'] = replies['attack'].astype(str) + '-' + replies['rec-nition'].astype(str)\n",
    "\n",
    "#check how many of each\n",
    "replies['A/R'].value_counts(dropna=False) \n",
    "\n",
    "#Make a new dataframe with the new split \n",
    "con_1 = (replies['A/R'] == '1-1')\n",
    "df1_1 = replies[con_1].sample(1000)\n",
    "\n",
    "con_2 = (replies['A/R'] == '1-0')\n",
    "df1_0 = replies[con_2].sample(3000)\n",
    "\n",
    "con_3 = (replies['A/R'] == '0-0')\n",
    "df0_0 = replies[con_3].sample(3000)\n",
    "\n",
    "con_4 = (replies['A/R'] == '0-1')\n",
    "df0_1 = replies[con_4].sample(3000)\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "stratified_sample = pd.concat([df1_1, df1_0,df0_0,df0_1], axis=0)\n",
    "stratified_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65a4c8-32a9-4e5f-a64a-49a7333a8802",
   "metadata": {},
   "source": [
    "### Using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72173a9-473c-4a64-951e-6c80ab75b1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2117740019.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['preprocessed_text'] = replies['cleaned_text'].apply(lambda x: re.sub(r'@\\w+', '', x).strip())\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2117740019.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['preprocessed_text'] = replies['preprocessed_text'].str.lower()\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2117740019.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['preprocessed_text'] = replies['preprocessed_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "#Remove user data\n",
    "#e.g '@ninna1122 i dont agree with you' will be 'i dont agree with you'\n",
    "replies['preprocessed_text'] = replies['cleaned_text'].apply(lambda x: re.sub(r'@\\w+', '', x).strip())\n",
    "\n",
    "#lowercase\n",
    "replies['preprocessed_text'] = replies['preprocessed_text'].str.lower()\n",
    "\n",
    "#remove special charachters \n",
    "replies['preprocessed_text'] = replies['preprocessed_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5dae4-ed5b-421c-baf0-a9525198de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04929308-26bc-4c2e-a22c-4af349adcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test df\n",
    "test = replies.head(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff07868-a016-4125-a51d-4c8f6dab7320",
   "metadata": {},
   "source": [
    "#### TF-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15bc8d1-d6ae-4af1-aa43-3514982854bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/3244366749.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['tfidf_cos_sim_hate_sentence'] = cosine_similarities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['du dum at h√∏re p√•',\n",
       " 'du er dum at h√∏re p√•',\n",
       " 'du er s√• dum at h√∏re p√•',\n",
       " 'du er jo bare dum at h√∏re p√•',\n",
       " 'hvor er du dog dum at h√∏re p√•',\n",
       " 'jeg hader dig',\n",
       " 'jeg hader dig',\n",
       " 'jeg hader dig',\n",
       " 'at du er dum at h√∏re p√• er ikke  n√∏dvendigvis  det samme som at du er dum',\n",
       " 'nej det er jeg ikke men du er stadig dum at h√∏re p√•',\n",
       " 'jeg hader dig ikkeüíêüòäüòò',\n",
       " 'jeg hader dig ikke ',\n",
       " 'sikke en egoistisk idiot üôÑ',\n",
       " 'nu er du bare dum at h√∏re p√• og du ved det nok godt',\n",
       " 'üò∂ du er dum',\n",
       " 'du er dum',\n",
       " 'fuck jeg hader dig \\njeg √¶der det jo ikke din idiot',\n",
       " 'er du dum eller hvad er jeg ogs√• tr√¶t af at h√∏re p√•',\n",
       " 'du er virkelig dum at h√∏re p√• ole',\n",
       " 'sikke en üòä']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#cosine similarity to hate sentence ('Du for dum at h√∏re p√•. Sikke en idiot. Jeg hader dig')\n",
    "\n",
    "v= TfidfVectorizer()\n",
    "X = v.fit_transform(replies['preprocessed_text'])\n",
    "\n",
    "# Transform the snetence\n",
    "sentence = \"du for dum at h√∏re p√• sikke en idiot jeg hader dig\"\n",
    "sentence_tfidf = v.transform([sentence])\n",
    "\n",
    "# Calculate cosine similarity between the sentence and each sentence\n",
    "cosine_similarities = cosine_similarity(X, sentence_tfidf)\n",
    "\n",
    "# Output the cosine similarities\n",
    "replies['tfidf_cos_sim_hate_sentence'] = cosine_similarities\n",
    "\n",
    "# Sort the DataFrame by cosine similarity in descending order\n",
    "sorted_replies = replies.sort_values(by='tfidf_cos_sim_hate_sentence', ascending=False)\n",
    "\n",
    "# Select the top 4000 entries\n",
    "top_replies_hate = sorted_replies.head(4000)\n",
    "\n",
    "# Print the results and score\n",
    "#print(top_replies_hate[['text', 'tfidf_cos_sim_hate_sentence']].head(10))\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_hate['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c71b7a-c5c1-4ada-8987-363d8c72fc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/1586220956.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['tfidf_cos_sim_prosocial_sentence'] = cosine_similarities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s√• forst√•r jeg ikke din kommentar',\n",
       " 'det er faktuelt forkert',\n",
       " 'det skriver jeg nu heller ikke ikke forkert men skal vi sige bidrager til ',\n",
       " 'jeg forst√•r godt din frustration og din pointe det er slet ikke det httpstco8wvbvpuuv9',\n",
       " 'forst√•r ikke  eller misforst√•r',\n",
       " 'hvad bidrager du mon til med den kommentar t√¶nker du',\n",
       " 'den gode tone du ved üòâ',\n",
       " 'den kommentar forst√•r jeg ikke helt hvad er der galt',\n",
       " 'nu er der mig der ikke forst√•r  alts√• hvad betyder din kommentar',\n",
       " 'det er faktuelt forkert det du skriver',\n",
       " 'er det dinü§î',\n",
       " 'er det din üòä',\n",
       " 'er det din',\n",
       " 'er det din',\n",
       " 'er det dinüôÑ',\n",
       " 'er det din üòò',\n",
       " 'er det din',\n",
       " 'er det din üòâ',\n",
       " 'er det din ‚ù§Ô∏è',\n",
       " 'er det din üò≥']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cosine similarity to prosocial sentence ('Jeg forst√•r din frustration, men. Lad os huske den gode tone..Jeg finder ikke din kommentar s√¶rlig konstruktiv. Vi har m√•ske forskellige synspunkter, men.. Had bidrager ikke til den gode debat. Dette er ikke sandt, det er faktuelt forkert, du misforst√•r hvad.. ')\n",
    "\n",
    "v= TfidfVectorizer()\n",
    "X = v.fit_transform(replies['preprocessed_text'])\n",
    "\n",
    "# Transform the sentence\n",
    "sentence = \"jeg forst√•r din frustration men lad os huske den gode tone jeg finder ikke din kommentar s√¶rlig konstruktiv vi har m√•ske forskellige synspunkter men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforst√•r hvad\"\n",
    "sentence_tfidf = v.transform([sentence])\n",
    "\n",
    "# Calculate cosine similarity between the sentence and each sentence\n",
    "cosine_similarities = cosine_similarity(X, sentence_tfidf)\n",
    "\n",
    "# Output the cosine similarities\n",
    "replies['tfidf_cos_sim_prosocial_sentence'] = cosine_similarities\n",
    "\n",
    "# Sort the DataFrame by cosine similarity in descending order\n",
    "sorted_replies = replies.sort_values(by='tfidf_cos_sim_prosocial_sentence', ascending=False)\n",
    "\n",
    "# Select the top 4000 entries\n",
    "top_replies_prosocial = sorted_replies.head(4000)\n",
    "\n",
    "# Print the results and score\n",
    "#print(top_replies_prosocial[['text', 'tfidf_cos_sim_prosocial_sentence']].head(10))\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_prosocial['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53180c03-5ce6-44e8-be9d-ae614871e649",
   "metadata": {},
   "source": [
    "#### Pre-tranied BERT-model (on prosocial sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f259d2-63b0-4754-8524-e97596241018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [13:14<00:00, 25.17it/s]\n",
      "Calculating Cosine Similarities: 100%|‚ñà‚ñà| 20000/20000 [00:02<00:00, 9026.30it/s]\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/4066471403.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['BERT_cos_sim_prosocial_sentence'] = cosine_similarities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alts√• jeg er enig i det du kritiserer men det er vel strengt taget ikke det han tweeter som jeg l√¶ser det bruger han anledningen ck til at romantisere ikke en sandsynlighedsberegning',\n",
       " 'nej jeg tror ikke du misforst√•r og jeg forst√•r ogs√• godt hvad du mener p√• et teoretisk plan fungerer dit svar ogs√• 100 jeg problematiserer det ogs√• mere p√• et praktisk niveau det var bare en lille brain fart der overkom mig',\n",
       " 'er det et langsigtet behov at g√∏re st√∏rstedelen af befolkningen afh√¶ngig af det offentlige jeg forst√•r slet ikke din argumentation for mig at se er velf√¶rd lig med skumfidusen og vi spiser den alle sammen  med det samme',\n",
       " 'sikke da et plat og i √∏vrigt forkert tweet vi er ikke tavse vi er forargede vi er imod snyd og kriminalitet lad venligst v√¶re med at p√•st√• andet',\n",
       " 'lige en lille opfordring vil du ikke nok s√¶tte dig bare lidt ind i tingene inden du forts√¶tter med komme med udokumenterede p√•stande \\ndet er helt ok at have en anden mening men at du konsekvent spreder den ene usaglige p√•stand efter den anden er decideret usmageligt',\n",
       " 'hvad bilder du dig ind misbruger egen vinding jeg kan selvf√∏lgelig sp√∏rge mit embedsv√¶rk til effekten af pr√¶cis de forslag jeg vil det beh√∏ver ikke engang at v√¶re nogle jeg selv g√•r ind for fair at du synes at jeg er en ond liberalist men fri mig for det andet',\n",
       " '   fordt√•r ikke hvorfor du er s√• urimelig jeg svarer p√• sp√∏rgsm√•l men keg hr andre holdninger end dig',\n",
       " 'fair nok hvis du ikke kan li manden og hans holdninger men jeg syntes du b√∏r holde den slags ufunderede spekulationer for dig selv det er lidt d√•rlig stil',\n",
       " 'jeg forst√•r simpelthen ikke hvorfor det er s√• sv√¶rt at respektere andres religion  hvorfor skal hun overhovedet have taletid √∏v jeg synes bare det er s√• fucking trist at hun kun f√•r det til at handle om sin egen retf√¶rdighedsf√∏lelse',\n",
       " 'jeg mener du har misforst√•et hvad privat ejendomsret er ejendomret har ingen gr√¶nser for hvad  eller med hvem man ejer noget med pointen er at ejendom er privat og man selv kan betemme hvad man vil eje jeg tror denne ret vil blive afskaffet med din demokratiske socialisme',\n",
       " 'l√¶s min bog eller en hvilken som helst artikel jeg har skrevet og du vil se at jeg ikke st√∏tter regimet at jeg har brugt hele min karriere p√• at kritisere regimet og udstille dens undertrykkelse jeg ved godt nuancer g√∏r ondt i din sorthvide verden men du er alts√• ynkelig',\n",
       " 'jeg tror du er naiv hvis du tror ren socialisme f√∏rer til noget godt jeg beh√∏ver blot at se p√• debat√∏res hadske retorik og jeg kan konstatere at hadet kun kan f√∏rer et sted hen hvis du vil afskaffe rigdom s√• kan du kun g√∏re det med totalit√¶re midler punktum',\n",
       " 'jeg ved ikke rigtigt hvad der er ventreorienteret og h√∏jreorienteret mere det er hele debatten der er fordrejet og alle kan ikke se at kongen ikke har noget t√∏j p√• bortset fra den lille dreng det er synsbedrag af st√∏rste klasse',\n",
       " 'hvad er dine kvalifikationer til at vurdere min ‚Äúempiri‚Äù ligesom jeg ikke generelt g√•r rund og g√∏r mig klog p√• hvad journalister kan og ikke kan baseret p√• min begr√¶nsede erfaringer hvad giver s√• dig selvtilliden til at mene at jeg tager fejl',\n",
       " 'du citerer ‚Äúisraelske og arabiske eksperter‚Äù men udtaler dig skr√•sikkert om iran klart jeg ikke kan vide hvad du baserer din viden om iran p√• n√•r jeg tilf√¶ldigvis ikke kender til dig beklager at du blev s√• forn√¶rmet ikke meningen god s√∏ndag',\n",
       " 'tak for svar men det er jo ikke √•rsagen jeg klandrer jeg for det er jeres manglende ux og kundeinformation det er under al kritik',\n",
       " 'jeg tror godt du forst√•r minimumsnormeringer er politisk l√¶ngere er den ikke n√•r du smider kortet om at nogen m√• give sig s√• er det fordi forslaget er politisk \\nmin pointe er institutioner ikke b√∏r styres politisk de burde selv kunne finde ud af det',\n",
       " 'du er ikke den eneste vi er bare nogle der slet ikke kan tage det jeg m√• pr√∏ve at t√¶nke mere over hvorfor det irriterer mig kan ikke kun v√¶re fordi jeg har en ba i persisk sprog og kultur',\n",
       " 'det er dig der retweeter mig bygger en str√•mand og s√• n√¶gter at forholde dig til substansen n√•r jeg tager mig tid til at svare men det var da godt at nogen tager det alvorligt  s√• basalt et sp√∏rgsm√•l af s√• stor betydning utroligt du som analytiker ikke kan forst√• det',\n",
       " 'du er som sagt flere gange hjertens velkommen til at unfollow jeg har heller ikke synderlig interesse i at have en follower der siger jeg st√∏tter regimet n√•r de absolut intet ved om mig mit virke og min forskning farvel']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs quite allright on the test data: but slow (7 hr on whole dataset)\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "# Apply tqdm to the apply function to track progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# Use the apply function with progress tracking\n",
    "sentence_embeddings = test['preprocessed_text'].progress_apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "sentence =\"jeg forst√•r din frustration men lad os huske den gode tone jeg finder ikke din kommentar s√¶rlig konstruktiv vi har m√•ske forskellige synspunkter men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforst√•r hvad\"\n",
    "prosocial_sentence_embedding = get_bert_embedding(sentence, tokenizer, model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity between the word \"politik\" and each sentence with progress tracking\n",
    "cosine_similarities = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(sentence_embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "# Output the cosine similarities\n",
    "test['BERT_cos_sim_prosocial_sentence'] = cosine_similarities\n",
    "\n",
    "# Sort the DataFrame by cosine similarity in descending order\n",
    "sorted_replies = test.sort_values(by='BERT_cos_sim_prosocial_sentence', ascending=False)\n",
    "\n",
    "# Select the top 15 entries\n",
    "top_replies = sorted_replies.head(30)\n",
    "\n",
    "# Print the results\n",
    "#print(top_replies[['text', 'BERT_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208782c-a50d-4fca-9387-8889d3b469bc",
   "metadata": {},
   "source": [
    "#### XLM-RoBERTa (on prosical sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed0346e-a5d4-4c73-8578-c141a1dd6d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generating Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [11:35<00:00, 28.77it/s]\n",
      "Calculating Cosine Similarities: 100%|‚ñà‚ñà| 20000/20000 [00:02<00:00, 8095.84it/s]\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/1634502232.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['roberta_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['almindelige mennesker har heller ikke r√•d til at bo alle steder jeg gad godt bo p√• strandvejen men det har jeg ikke r√•d til skal vi s√• til at lave en lov om det \\n\\ndesuden kan du ikke renovere lejlighed hvor du bor i forvejen  det kun n√•r den st√•r tom muligheden opst√•r',\n",
       " 'jeg stiller altid op hvorn√•rsomhelst til debat ang iran men der er ik ngt at misforst√• her jeg synes det er et meget relevant og vigtigt spm at stille om der var specifik trussel mod danske skibe f√∏r snak om maritim indsats det g√∏r puck ikke',\n",
       " 'det er dig der retweeter mig bygger en str√•mand og s√• n√¶gter at forholde dig til substansen n√•r jeg tager mig tid til at svare men det var da godt at nogen tager det alvorligt  s√• basalt et sp√∏rgsm√•l af s√• stor betydning utroligt du som analytiker ikke kan forst√• det',\n",
       " 'jeg tror godt du forst√•r minimumsnormeringer er politisk l√¶ngere er den ikke n√•r du smider kortet om at nogen m√• give sig s√• er det fordi forslaget er politisk \\nmin pointe er institutioner ikke b√∏r styres politisk de burde selv kunne finde ud af det',\n",
       " 'du misforst√•r hvad borgerlige vil de g√•r ikke imod normeringer de g√•r ind for afregulering og lade pengene f√∏lge barnet s√• kan institutionerne konkurrere om at levere den bedste service men det forst√•r  ikke',\n",
       " 'men pas nu p√• med at hop p√• at det er forslag jeg har ikke stillet de 49 forslag nogle af dem er selvf√∏lgelig kendt som lapositioner andre er bestemt ikke men forst√•r godt det kan misforst√•s n√•r  stiller det s√•dan op',\n",
       " 'men der passer jo heller ikke som det er fremg√•et s√• mener jeg at man skal sikre kongressens sikkerhed men at man normaliserer det radikaliserede ved at lade den afholde i folketinget der er ingen mods√¶tning mellem de to synspunkter',\n",
       " 'tror du ikke vi andre er interesserede i fakta du udviser absolut ingen grad ad lydh√∏r bare din egen altoverskyggende bedrevidenhed og komplette blinde tillid til en hel stand  i s√•dan en grad at du tillader dig at kalde andre l√∏gnere',\n",
       " ' ikkeakademiske arbejdsformer men derfor kan vi stadig godt diskutere teori p√• et abstrakt plan med brug af komplicerede s√¶tninger det er helt fint at kunne begge dele og lidt af en str√•mand at sige at vi kun uddanner til det ene',\n",
       " 'jeg er med p√• at voldelig revolution ikke er del af agendaen men nu snakker du om indvandring jeg er totalt forvirret \\n\\nvenstrefl√∏jen hader rige mennesker ja eller nej',\n",
       " 'eller trykkede p√• billedet og l√¶ste navnet der jeg er nok bare ret d√•rlig til at gemme mig men rigtig mange tak for den konstruktive r√•d der desv√¶rre ikke flytter fokus v√¶k fra budskabet som det ellers var meningen',\n",
       " 'lige nu ligger dens v√¶rdi kun i at folk tror den vil stige i v√¶rdi\\np√• et eller andet tidspunkt falder den nok kraftigt og stabiliserer sig s√• ejerne rent faktisk begynder at bruge den men indtil da er det blot en ikkevaluta',\n",
       " 'vi er ikke uenige om hvorvidt man skal sikre dem at holde et m√∏de lad os lige holde fast i det \\njeg forst√•r bare ikke at man derfor skal give dem adgang til at holde kongres i folketinget jeg tror at det vil medvirke til normalisering af de ekstreme',\n",
       " 'jeg har kun set highlights men det er jo helt sk√∏rt at okore sparker kliment bagefter og smadre hans korsb√•nd jeg er sikker p√• at han ikke har intention om at skade ham men stadigv√¶k ikke i orden at lave s√•dan en h√¶vn aktion',\n",
       " 't√¶nker at i lige kan vende den her i n√¶ste afsnit det er da godt nok ved at v√¶re lidt for plat hvad de har gang i over p√• heden jeg ville v√¶re pinlig hvis jeg var dem m√•ske h√∏re hvad deres fangruppe eller fanpodcast synes',\n",
       " 'desuden kryber du igen ud om det tweetet handlede om og som jeg konstant minder dig om  pr√¶missen om at vi som princip uddanner folk til at skrive d√•rligt nu siger du s√• det var en sp√∏g s√• du er alts√• enig i at det er forkert',\n",
       " 'tak for svar jeg er helt med p√• at det er en udfordring selvom odense lader til at have f√•et styr p√• det men jeg h√•ber at i vil sl√• ned p√• selskaber der konsekvent bryder reglerne og fjerne deres tilladelse i hvert fald indtil de uddanner deres ansatte',\n",
       " 'det er der ved at blive og engagere mig kan jeg med masser af held blive h√∏rt i partiet med tiden derudover kan jeg st√∏tte de socialdemokratiske kr√¶fter der stadig sidder h√∏jt i partiet jeg har for eksempel ikke t√¶nkt mig at holde k√¶ft p√• kongressen',\n",
       " 'jeg er ikke ekspert i den slags og eksperterne er s√• vidt jeg ved uenige det kan passe det kan v√¶re disinformation  hvem ved iranerne handler ofte ulovligt s√¶rligt n√•r de er presset op i en krog',\n",
       " 'jeg fastholder at det der st√•r i artiklen er en forklaring p√• hvorfor i reagerede som trump ikke en reel undskyldning s√• m√• du tale med journalisten hvis du mener at du er fejlciteret eller dit sidste citat er taget ud af kontekst']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decent on the test data (takes also 7hr on full dataset)\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "# Function to get RoBERTa embeddings\n",
    "def get_roberta_embedding(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "\n",
    "# Generate embeddings for DataFrame\n",
    "embeddings = [get_roberta_embedding(text, tokenizer, model) for text in tqdm(test['preprocessed_text'], desc=\"Generating Embeddings\")]\n",
    "\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "sentence = \"jeg forst√•r din frustration men lad os huske den gode tone jeg finder ikke din kommentar s√¶rlig konstruktiv vi har m√•ske forskellige synspunkten men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforst√•r hvad\"\n",
    "prosocial_sentence_embedding = get_roberta_embedding(sentence, tokenizer, model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "test['roberta_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "\n",
    "# Sort and display top results\n",
    "sorted_df = test.sort_values(by='roberta_cos_sim_prosocial_sentence', ascending=False)\n",
    "top_replies = sorted_df.head(30)\n",
    "\n",
    "#print(top_replies[['text', 'roberta_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c76df-5385-4a34-b269-df7533acf982",
   "metadata": {},
   "source": [
    "#### Fasttext (an extension of Word2Vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9871e7da-75c3-4084-9ff9-509e31972661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 642613/642613 [00:56<00:00, 11461.60it/s]\n",
      "Calculating Cosine Similarities: 100%|‚ñà| 642613/642613 [01:17<00:00, 8241.49it/s\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2306015175.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['fasttext_cos_sim_hate_sentence'] = [cosine_similarity([embedding], hate_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['du er sgu endnu engang for dum til at jeg gider at bruge tid p√• dig',\n",
       " 'du har da ikke selv holdt dig for god til at anmelde hvad du vurderede at v√¶re en falsk reklame s√• m√•ske skulle du finde en mindre hest at sidde p√•',\n",
       " 'nu slukker jeg sgu for dig for du er virkelig irriterende at h√∏re p√•',\n",
       " 'nu er du bare dum at h√∏re p√• her troede man at man kunne have en alm diskussion med en voksen mand trist',\n",
       " 'shit da en r√∏vlort \\njeg er fandeme ked af at du har set dig n√∏dsaget til at g√∏re dette og gal for helvede men jeg h√•ber det betyder at du f√•r en bedre oplevelse her\\nsikke en lortestodder',\n",
       " 'at du sender mig en tanke n√•r du ser en smuk pige spille fiol har jeg intet ondt at sige om \\ngodmorgen',\n",
       " 'h√•ber du er okay lad mig vide hvis du skal bruge en skulder at gr√¶de p√•',\n",
       " 'du er fanme da for dum at h√∏re p√• jeg h√•ber da at ulven tager sig k√¶rligt af dig og dine s√• f√•r piben en anden lyd',\n",
       " 'jeg tror du tvivler p√• alt fordi du synes den her kamp er en du lige skal blande dig i uden at t√¶nke p√• andre tak for solidariteten kammerat',\n",
       " 'stop dig selv kathe gider ikke at h√∏re p√• forsvar for s√• latterlig en kommentar',\n",
       " 'hold nuk√¶ft hvor er du dum at h√∏re p√•  h√∏jrefl√∏js trolls som dig fortjener ikke svar',\n",
       " 'du form√•r fint at vide at du ikke har fattet en skid en moderne lad dem spise kage idioti',\n",
       " 'ja for du rammer aldrig forkert p√• tasterne det er en joke din klump undskyld jeg forn√¶rmede din religion btw vi snakkede om at se fodbold du ved at det ikke giver motion ikke üòâ',\n",
       " 'og du er en iriterende dum g√•s drop at diskutere med mig hvis du ikke kan v√¶re seri√∏s',\n",
       " 'shit du er en landsbytosse at h√∏re p√• hjem under stenen med dig',\n",
       " 'arghhhhh tr√¶ls at du kalder det udskamning at jeg giver udtryk for en anden holdning',\n",
       " 'hahaha du er en kvinde af f√• ord du forventer at se dig p√• vores arrangement  har du v√¶ret p√• arendalsuka f√∏r for s√• forventer jeg mig at nordmanden i dig kan guide mig lidt ',\n",
       " 'for mig eller dig du kan heldigvis v√¶lge at skippe mine tweets  jeg havde foretrukket at l√¶se bogen p√• en ferie',\n",
       " 'jaaa mand √∏nsker mig kun at du breaker svaret en fredag aften ',\n",
       " 'du siger jeg har lige googlet dig det forklarer en del efterfulgt af jeg d√∏mmer dig ikke üòÖer da glad for at du kan se din m√•de at debattere p√•  som jeg har parodieret  er st√¶rkt barnlig og uvirksom ift at f√• din pointe frem og alts√• ret komisk \\nhttpstcok7zexxahch']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hate sentence embeddings, on whole dataset\n",
    "\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the pretrained FastText model: takes 5 min to load: uncomment after first run\n",
    "fasttext_model = load_facebook_model('/Users/idahelenedencker/Desktop/STANDBY_Ida/Creating dataset of reference tweets/Models/cc.da.300.bin')\n",
    "\n",
    "# Function to get FastText embeddings\n",
    "def get_fasttext_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_embeddings = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not word_embeddings:  # If no words in the sentence are in the vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Generate embeddings for DataFrame\n",
    "embeddings = [get_fasttext_embedding(text, fasttext_model) for text in tqdm(replies['preprocessed_text'], desc=\"Generating Embeddings\")]\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "hate_sentence = \"du for dum at h√∏re p√• sikke en idiot jeg hader dig\"\n",
    "hate_sentence_embedding = get_fasttext_embedding(hate_sentence, fasttext_model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "replies['fasttext_cos_sim_hate_sentence'] = [cosine_similarity([embedding], hate_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "# Sort and display top results\n",
    "sorted_df = replies.sort_values(by='fasttext_cos_sim_hate_sentence', ascending=False)\n",
    "top_replies_hate = sorted_df.head(4000)\n",
    "\n",
    "#print(top_15_replies[['text', 'fasttext_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_hate['preprocessed_text'].head(20).tolist()\n",
    "text_to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04eadd5e-c5be-4059-b7d3-19e5dc1cbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 642613/642613 [00:51<00:00, 12401.56it/s]\n",
      "Calculating Cosine Similarities: 100%|‚ñà| 642613/642613 [01:16<00:00, 8429.34it/s\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/449282597.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['fasttext_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jeg siger det vel strengt taget bare til dem der f√∏lger mig  men hvis du f√∏lger den efterf√∏lgende diskussion s√• er jeg netop ikke imponeret over den tilgang spillerne har',\n",
       " 'had alt det du vil men du skal ikke g√∏re dig til dommer over hvad jeg mener er en naturlig reaktion jeg d√¶kker overhovedet ikke den sag',\n",
       " 'din f√∏rste indvending er muligvis korrekt det skal vurderes den anden forst√•r jeg ikke hvilke andre vacciner tilbyder vi offlabel',\n",
       " 'hvis det ovenik√∏bet er er kendt adf√¶rd s√• forts√¶tter den jo udelukkende fordi ingen tager den videre  han kan jo desv√¶rre ikke se det  hvis du ikke √∏nsker den store sag  s√• tror jeg du skal g√∏re mere ved selve konfrontationen med ham  giv din vrede meregas  for din skyld ‚ô•Ô∏è',\n",
       " 'det skrev jeg ikke men uanset hvad jeg kaster efter dig vil du jo ikke godtage det  jeg har ikke en blind tro men jeg vurdere efter bedste evne at man har gjort det rigtige med fejl og alt muligt undervejs klart men grundl√¶ggende den rigtige respons',\n",
       " 'tror det er den gamle lortesag mange refererer til og det skal man ikke joke med sp√∏rger du mig ',\n",
       " 'okay trine skal vi √•bne for dine sp√∏rgsm√•l til tenet jeg mener faktisk ikke den er s√• uigennemskuelig endda',\n",
       " 'det er jeg ked af jeg vil gerne forst√• din bekymring  det er derfor jeg svarer dig  men jeg har desv√¶rre lidt sv√¶rt ved at f√∏lge dit r√¶sonnement der lyder som om vi snakker forbi hinanden m√•ske er det twitter jeg ved det ikke men tager altid gerne en seri√∏s dialog',\n",
       " 'lidt men jeg vil faktisk rigtig gerne forst√• din kritik  vi rammer ikke bredt nok skriver du hvordan',\n",
       " 'vi opfatter nok bare debatten forskelligt jeg synes blot du understreger min pointe selvom det n√¶ppe er din hensigt ü§∑üèª\\u200d‚ôÄÔ∏è',\n",
       " 'skal jeg tage din med n√•r jeg er dernede jeg respekterer du er uenig med mig men ikke din tone det er un√∏dvendigt',\n",
       " 'jamen jeg forst√•r dine holdninger p√• omr√•det qua din tilknytning og det er absolut fair nok üòâ\\n\\nmen demokratisk vedtagne love skal ikke f√∏lges hvis vi bare f√∏ler noget andet \\n\\ner d den form for politiker du vil v√¶re hvis du opn√•r mandat\\n\\nbegge dele skrevet med respekt',\n",
       " 'selvf√∏lgelig kan jeg det men jeg vil dog ikke lade det afspore samtalen fra substansen som jeg ser du g√∏r her selvom din intention er udem√¶rket jeg tager afstand fra racistiske udtalelser og ideer og √∏nsker ikke at sugarcoate dem med at vi er gode mennesker alligevel',\n",
       " 'n√¶√¶√¶ det vidste jeg ikke det jeg siger er bare at lovlig vigtigt emne til d√©n type twittersk√¶nderier er nok heller ikke den direkte vej til mere sammenh√¶ngsamarbejde og svaret p√• dit sp√∏rgsm√•l har vi vel i√∏vrigt forl√¶ngst givet ü§î',\n",
       " 'fair pointe konspirationsteori er nok til den gode sideüòÇ jeg er ikke ekspert i hvordan og hvor l√¶nge teledata opbevares inden det overskrives m√•ske du kan g√∏re os klogere her',\n",
       " 'tak for diskussionen nikolaj vi kommer det ikke videre jeg har skrevet hvad jeg mener men jeg er faktisk stadig ikke helt klar over hvad du mener jeg har gjort forkert  og hvad dit argumentbev√¶ggrund for forklare patriots ageren i sagen er',\n",
       " 'seri√∏s journalistik enig p√•standen er jo bare vanvittig eftersom det netop er en p√•stand blandt alt for mange andre godt den infantile adf√¶rd udfordres men lad os b√¶re over har l√¶st at hun kan lide br√∏ndby',\n",
       " 'jeg aner ikke hvad din dr√∏m er men jeg h√•ber du g√∏r dem alle til skamme ',\n",
       " 'det opsummerer det hele godt kald os bare gamle sure m√¶nd jeg vil kalde det rettidigt omhu h√•ber inderligt du f√•r ret men jeg ville kalde vores sportslige ledelse decideret naiv hvis de planl√¶gger p√• den slags forh√•bninger der er alt for stor sandsynlighed for det g√•r galt',\n",
       " 'den er ikke forkert hvis vi bare holder os til det fodboldm√¶ssige üòÄ jeg tror det ville v√¶re godt for alle hvis han snart blev skudt afsted']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prosocial sentence embeddings, on whole dataset\n",
    "\n",
    "\n",
    "# Function to get FastText embeddings\n",
    "def get_fasttext_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_embeddings = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not word_embeddings:  # If no words in the sentence are in the vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Generate embeddings for DataFrame\n",
    "embeddings = [get_fasttext_embedding(text, fasttext_model) for text in tqdm(replies['preprocessed_text'], desc=\"Generating Embeddings\")]\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "prosocial_sentence = \"jeg forst√•r din frustration men lad os huske den gode tone jeg finder ikke din kommentar s√¶rlig konstruktiv vi har m√•ske forskellige synspunkten men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforst√•r hvad\"\n",
    "prosocial_sentence_embedding = get_fasttext_embedding(prosocial_sentence, fasttext_model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "replies['fasttext_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "# Sort and display top results\n",
    "sorted_df = replies.sort_values(by='fasttext_cos_sim_prosocial_sentence', ascending=False)\n",
    "top_replies_prosocial = sorted_df.head(4000)\n",
    "\n",
    "#print(top_15_replies[['text', 'fasttext_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_prosocial['preprocessed_text'].head(20).tolist()\n",
    "text_to_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82057bb1-34cc-4a02-af53-e10d71099931",
   "metadata": {},
   "source": [
    "### Using Fasttext embeddings (20 k sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "111c666b-e047-4f32-b92f-4ae41430254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 8000/8000/4000 split: using fasttext\n",
    "\n",
    "#delete tfidf_cos_sim_hate_sentence and tfidf_cos_sim_prosocial_sentence columns\n",
    "replies = replies.drop(['tfidf_cos_sim_hate_sentence', 'tfidf_cos_sim_prosocial_sentence'], axis=1)\n",
    "\n",
    "#make dataframes based on cosine similarity scores \n",
    "sorted_df_prosocial = replies.sort_values(by='fasttext_cos_sim_prosocial_sentence', ascending=False).head(8000)\n",
    "sorted_df_hate = replies.sort_values(by='fasttext_cos_sim_hate_sentence', ascending=False).head(8000)\n",
    "\n",
    "#make random dataframe\n",
    "sorted_df_random = replies.sample(4000)\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "word_embeddings_fasttext = pd.concat([sorted_df_prosocial, sorted_df_hate,sorted_df_random], axis=0)\n",
    "word_embeddings_fasttext\n",
    "\n",
    "#save\n",
    "word_embeddings_fasttext.to_csv('/Users/idahelenedencker/Desktop/STANDBY_Ida/Creating dataset of reference tweets/replies_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
