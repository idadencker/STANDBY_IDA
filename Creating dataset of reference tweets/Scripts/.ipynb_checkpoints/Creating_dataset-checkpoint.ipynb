{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19738d37-1769-465f-a414-eeea3a606059",
   "metadata": {},
   "source": [
    "# Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4435c95d-c8f6-4088-b9dd-a395192b19e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (6.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#load pacakges \n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install torch \n",
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2b258-4de2-4a97-a563-a58e2e166a87",
   "metadata": {},
   "source": [
    "# Loading, cleaning and filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7a94d8-d507-463f-b6a1-3e533ad8ffb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conversation_id                  object\n",
       "lang                             object\n",
       "created_at                       object\n",
       "id                               object\n",
       "text                             object\n",
       "author_id                        object\n",
       "public_metrics.retweet_count      int64\n",
       "public_metrics.reply_count        int64\n",
       "public_metrics.like_count         int64\n",
       "public_metrics.quote_count        int64\n",
       "referenced_tweets_id             object\n",
       "referenced_tweets_type           object\n",
       "in_reply_to_user_id              object\n",
       "__index_level_0__               float64\n",
       "PNR                              object\n",
       "surveyXact_externke              object\n",
       "non_unique_twitter_author_id    float64\n",
       "started_survey                  float64\n",
       "cleaned_text                     object\n",
       "rec-nition                        int64\n",
       "attack                            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "# Define the dtype dictionary\n",
    "dtype_dict_all = {\n",
    "    'conversation_id': 'object',\n",
    "    'lang': 'object',\n",
    "    'created_at': 'object',\n",
    "    'id': 'object',\n",
    "    'text': 'object',\n",
    "    'author_id': 'object',\n",
    "    'referenced_tweets_id': 'object',\n",
    "    'referenced_tweets_type': 'object',\n",
    "    'in_reply_to_user_id': 'object',\n",
    "    'PNR': 'object',\n",
    "    'surveyXact_externke': 'object',\n",
    "    'cleaned_text': 'object'\n",
    "}\n",
    "\n",
    "# Load in the scored csv file with the dictionary\n",
    "scored_tweets= pd.read_csv('/Users/idahelenedencker/Desktop/STANDBY_Ida/Rec-nition and attack on twitter data/CSV files/twitter_w_rec_attack_final.csv',  dtype=dtype_dict_all)\n",
    "\n",
    "#check data types\n",
    "scored_tweets.dtypes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773fbcef-08b1-45ee-911b-767c0f5fb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert non_unique_twitter_author_id and started_survey into integer\n",
    "scored_tweets['non_unique_twitter_author_id'] = pd.to_numeric(scored_tweets['non_unique_twitter_author_id'], errors='coerce').astype('Int64')\n",
    "scored_tweets['started_survey'] = pd.to_numeric(scored_tweets['started_survey'], errors='coerce').astype('Int64')\n",
    "\n",
    "#check value counts\n",
    "scored_tweets['rec-nition'].value_counts(dropna=False) \n",
    "scored_tweets['attack'].value_counts(dropna=False) \n",
    "\n",
    "#filter to replied_to\n",
    "replies = scored_tweets[scored_tweets['referenced_tweets_type'] == 'replied_to']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beef1bc-ab82-4b0b-b51f-fb19a15657fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6d5b3-9331-4ecd-85c3-c5be08efadba",
   "metadata": {},
   "source": [
    "- conversation_id: the Post ID of the original Post that started the conversation. (if the post is not a Retweet, Quoted Tweet, or reply the conversation_id will be the same as id)\n",
    "- lang: language\n",
    "- created_at: when it was created (YYYY-MM-DDTHH:MM:SS.000Z, ISO8601 format giving: year, month, data at Time: hour:minute:second:milisecond in Coordinated Universal Time (UTC))\n",
    "- id: Each object within Twitter - a Tweet, Direct Message, User, List, and so on - has a unique ID\n",
    "- text: the post, starts with @ if it is a retweet or reply to\n",
    "- author_id: returns user object representing the Post's author\n",
    "- public_metrics.retweet_count: how many retweeted\n",
    "- public_metrics.reply_count: how many replied\n",
    "- public_metrics.like_count: how many liked\n",
    "- public_metrics.quote_count: how many quoted\n",
    "- referenced_tweets_id: returns post object that this Post is referencing (either as a Retweet, Quoted Tweet, or reply)\n",
    "- referenced_tweets_type: if it is a post (None), retweeted or replied_to etc.\n",
    "- in_reply_to_user_id: returns a user object representing the Post author this requested Post is a reply of\n",
    "- __index_level_0__: ??\n",
    "- PNR: personnummer (Eentydig identifikation af person), like cpr\n",
    "- surveyXact_externke: personification ID linked to the e-boks survey\n",
    "- non_unique_twitter_author_id (made by simon): ?? can take 0 or 1\n",
    "- started_survey: (made by simon): if they started the e-boks survey (doesnt mean finished it) can take 0 or 1\n",
    "\n",
    "\n",
    "\n",
    "Read more here: \n",
    "- https://developer.x.com/en/docs/twitter-ids (ID)\n",
    "- https://developer.x.com/en/docs/twitter-api/expansions (in_reply_to_user_id, author_id, referenced_tweets_id)\n",
    "- https://developer.x.com/en/docs/twitter-api/conversation-id (conversation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709e820-bdae-4317-8a68-3de84743591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at performance of algorithms\n",
    "\n",
    "#pull up text examples\n",
    "attack= replies[replies['attack'] == 1]\n",
    "print(attack['text'][:100])\n",
    "\n",
    "rec_nition= replies[replies['rec-nition'] == 1]\n",
    "print(rec_nition['text'][:100])\n",
    "\n",
    "# can print the whole text using\n",
    "text_to_print = rec_nition['text'].head(20).tolist()\n",
    "text_to_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498b404-8b21-4f06-b0d8-2e8592a5a3d2",
   "metadata": {},
   "source": [
    "# Limiting the df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75d2ac-62c9-4bd8-8c7b-2d8cbe2b20cb",
   "metadata": {},
   "source": [
    "## Random sampling (10k samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff0597-8a0b-482b-854a-ce540d0970d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sampling\n",
    "replies_random= replies.sample(10000)\n",
    "replies_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd221c6d-51e0-4f07-bd83-12006a5ff6b5",
   "metadata": {},
   "source": [
    "## Stratified sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca033f-d309-4a61-afbb-f8fb8e8d22d0",
   "metadata": {},
   "source": [
    "### Using attack/regnition split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b36199-947a-463d-93a5-38b5164c2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling using attack and rec-nition \n",
    "\n",
    "#idea: ¼ are scored 0-0 for attack-recnition; ¼ are scored 1-0 for attack-recnition, ¼ are scored 0-1 for attack-recnition, ¼ are scored 1-1 (possible that there are no/few examples that have both attack AND rec-nition labels) for attack-recnition \n",
    "\n",
    "# First: Check if there are any 1-1 cases\n",
    "condition = (replies['attack'] == 1) & (replies['rec-nition'] == 1)\n",
    "rows_with_condition = replies[condition]\n",
    "rows_with_condition #1156 cases (but not enough to be 25%)\n",
    "\n",
    "#new idea: 1000 of 1-1, 3000 1-0, 3000 0-1, 3000 0-0\n",
    "\n",
    "#make a new column 'A/R' with both scorings can take 0-0, 0-1, 1-1, or 1-0\n",
    "replies['A/R'] = replies['attack'].astype(str) + '-' + replies['rec-nition'].astype(str)\n",
    "\n",
    "#check how many of each\n",
    "replies['A/R'].value_counts(dropna=False) \n",
    "\n",
    "#Make a new dataframe with the new split \n",
    "con_1 = (replies['A/R'] == '1-1')\n",
    "df1_1 = replies[con_1].sample(1000)\n",
    "\n",
    "con_2 = (replies['A/R'] == '1-0')\n",
    "df1_0 = replies[con_2].sample(3000)\n",
    "\n",
    "con_3 = (replies['A/R'] == '0-0')\n",
    "df0_0 = replies[con_3].sample(3000)\n",
    "\n",
    "con_4 = (replies['A/R'] == '0-1')\n",
    "df0_1 = replies[con_4].sample(3000)\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "stratified_sample = pd.concat([df1_1, df1_0,df0_0,df0_1], axis=0)\n",
    "stratified_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65a4c8-32a9-4e5f-a64a-49a7333a8802",
   "metadata": {},
   "source": [
    "### Using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72173a9-473c-4a64-951e-6c80ab75b1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2117740019.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['preprocessed_text'] = replies['cleaned_text'].apply(lambda x: re.sub(r'@\\w+', '', x).strip())\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2117740019.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['preprocessed_text'] = replies['preprocessed_text'].str.lower()\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2117740019.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['preprocessed_text'] = replies['preprocessed_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "#Remove user data\n",
    "#e.g '@ninna1122 i dont agree with you' will be 'i dont agree with you'\n",
    "replies['preprocessed_text'] = replies['cleaned_text'].apply(lambda x: re.sub(r'@\\w+', '', x).strip())\n",
    "\n",
    "#lowercase\n",
    "replies['preprocessed_text'] = replies['preprocessed_text'].str.lower()\n",
    "\n",
    "#remove special charachters \n",
    "replies['preprocessed_text'] = replies['preprocessed_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5dae4-ed5b-421c-baf0-a9525198de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04929308-26bc-4c2e-a22c-4af349adcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test df\n",
    "test = replies.head(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff07868-a016-4125-a51d-4c8f6dab7320",
   "metadata": {},
   "source": [
    "#### TF-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15bc8d1-d6ae-4af1-aa43-3514982854bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/3244366749.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['tfidf_cos_sim_hate_sentence'] = cosine_similarities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['du dum at høre på',\n",
       " 'du er dum at høre på',\n",
       " 'du er så dum at høre på',\n",
       " 'du er jo bare dum at høre på',\n",
       " 'hvor er du dog dum at høre på',\n",
       " 'jeg hader dig',\n",
       " 'jeg hader dig',\n",
       " 'jeg hader dig',\n",
       " 'at du er dum at høre på er ikke  nødvendigvis  det samme som at du er dum',\n",
       " 'nej det er jeg ikke men du er stadig dum at høre på',\n",
       " 'jeg hader dig ikke💐😊😘',\n",
       " 'jeg hader dig ikke ',\n",
       " 'sikke en egoistisk idiot 🙄',\n",
       " 'nu er du bare dum at høre på og du ved det nok godt',\n",
       " '😶 du er dum',\n",
       " 'du er dum',\n",
       " 'fuck jeg hader dig \\njeg æder det jo ikke din idiot',\n",
       " 'er du dum eller hvad er jeg også træt af at høre på',\n",
       " 'du er virkelig dum at høre på ole',\n",
       " 'sikke en 😊']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#cosine similarity to hate sentence ('Du for dum at høre på. Sikke en idiot. Jeg hader dig')\n",
    "\n",
    "v= TfidfVectorizer()\n",
    "X = v.fit_transform(replies['preprocessed_text'])\n",
    "\n",
    "# Transform the snetence\n",
    "sentence = \"du for dum at høre på sikke en idiot jeg hader dig\"\n",
    "sentence_tfidf = v.transform([sentence])\n",
    "\n",
    "# Calculate cosine similarity between the sentence and each sentence\n",
    "cosine_similarities = cosine_similarity(X, sentence_tfidf)\n",
    "\n",
    "# Output the cosine similarities\n",
    "replies['tfidf_cos_sim_hate_sentence'] = cosine_similarities\n",
    "\n",
    "# Sort the DataFrame by cosine similarity in descending order\n",
    "sorted_replies = replies.sort_values(by='tfidf_cos_sim_hate_sentence', ascending=False)\n",
    "\n",
    "# Select the top 4000 entries\n",
    "top_replies_hate = sorted_replies.head(4000)\n",
    "\n",
    "# Print the results and score\n",
    "#print(top_replies_hate[['text', 'tfidf_cos_sim_hate_sentence']].head(10))\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_hate['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c71b7a-c5c1-4ada-8987-363d8c72fc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/1586220956.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['tfidf_cos_sim_prosocial_sentence'] = cosine_similarities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['så forstår jeg ikke din kommentar',\n",
       " 'det er faktuelt forkert',\n",
       " 'det skriver jeg nu heller ikke ikke forkert men skal vi sige bidrager til ',\n",
       " 'jeg forstår godt din frustration og din pointe det er slet ikke det httpstco8wvbvpuuv9',\n",
       " 'forstår ikke  eller misforstår',\n",
       " 'hvad bidrager du mon til med den kommentar tænker du',\n",
       " 'den gode tone du ved 😉',\n",
       " 'den kommentar forstår jeg ikke helt hvad er der galt',\n",
       " 'nu er der mig der ikke forstår  altså hvad betyder din kommentar',\n",
       " 'det er faktuelt forkert det du skriver',\n",
       " 'er det din🤔',\n",
       " 'er det din 😊',\n",
       " 'er det din',\n",
       " 'er det din',\n",
       " 'er det din🙄',\n",
       " 'er det din 😘',\n",
       " 'er det din',\n",
       " 'er det din 😉',\n",
       " 'er det din ❤️',\n",
       " 'er det din 😳']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cosine similarity to prosocial sentence ('Jeg forstår din frustration, men. Lad os huske den gode tone..Jeg finder ikke din kommentar særlig konstruktiv. Vi har måske forskellige synspunkter, men.. Had bidrager ikke til den gode debat. Dette er ikke sandt, det er faktuelt forkert, du misforstår hvad.. ')\n",
    "\n",
    "v= TfidfVectorizer()\n",
    "X = v.fit_transform(replies['preprocessed_text'])\n",
    "\n",
    "# Transform the sentence\n",
    "sentence = \"jeg forstår din frustration men lad os huske den gode tone jeg finder ikke din kommentar særlig konstruktiv vi har måske forskellige synspunkter men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforstår hvad\"\n",
    "sentence_tfidf = v.transform([sentence])\n",
    "\n",
    "# Calculate cosine similarity between the sentence and each sentence\n",
    "cosine_similarities = cosine_similarity(X, sentence_tfidf)\n",
    "\n",
    "# Output the cosine similarities\n",
    "replies['tfidf_cos_sim_prosocial_sentence'] = cosine_similarities\n",
    "\n",
    "# Sort the DataFrame by cosine similarity in descending order\n",
    "sorted_replies = replies.sort_values(by='tfidf_cos_sim_prosocial_sentence', ascending=False)\n",
    "\n",
    "# Select the top 4000 entries\n",
    "top_replies_prosocial = sorted_replies.head(4000)\n",
    "\n",
    "# Print the results and score\n",
    "#print(top_replies_prosocial[['text', 'tfidf_cos_sim_prosocial_sentence']].head(10))\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_prosocial['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53180c03-5ce6-44e8-be9d-ae614871e649",
   "metadata": {},
   "source": [
    "#### Pre-tranied BERT-model (on prosocial sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f259d2-63b0-4754-8524-e97596241018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [13:14<00:00, 25.17it/s]\n",
      "Calculating Cosine Similarities: 100%|██| 20000/20000 [00:02<00:00, 9026.30it/s]\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/4066471403.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['BERT_cos_sim_prosocial_sentence'] = cosine_similarities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['altså jeg er enig i det du kritiserer men det er vel strengt taget ikke det han tweeter som jeg læser det bruger han anledningen ck til at romantisere ikke en sandsynlighedsberegning',\n",
       " 'nej jeg tror ikke du misforstår og jeg forstår også godt hvad du mener på et teoretisk plan fungerer dit svar også 100 jeg problematiserer det også mere på et praktisk niveau det var bare en lille brain fart der overkom mig',\n",
       " 'er det et langsigtet behov at gøre størstedelen af befolkningen afhængig af det offentlige jeg forstår slet ikke din argumentation for mig at se er velfærd lig med skumfidusen og vi spiser den alle sammen  med det samme',\n",
       " 'sikke da et plat og i øvrigt forkert tweet vi er ikke tavse vi er forargede vi er imod snyd og kriminalitet lad venligst være med at påstå andet',\n",
       " 'lige en lille opfordring vil du ikke nok sætte dig bare lidt ind i tingene inden du fortsætter med komme med udokumenterede påstande \\ndet er helt ok at have en anden mening men at du konsekvent spreder den ene usaglige påstand efter den anden er decideret usmageligt',\n",
       " 'hvad bilder du dig ind misbruger egen vinding jeg kan selvfølgelig spørge mit embedsværk til effekten af præcis de forslag jeg vil det behøver ikke engang at være nogle jeg selv går ind for fair at du synes at jeg er en ond liberalist men fri mig for det andet',\n",
       " '   fordtår ikke hvorfor du er så urimelig jeg svarer på spørgsmål men keg hr andre holdninger end dig',\n",
       " 'fair nok hvis du ikke kan li manden og hans holdninger men jeg syntes du bør holde den slags ufunderede spekulationer for dig selv det er lidt dårlig stil',\n",
       " 'jeg forstår simpelthen ikke hvorfor det er så svært at respektere andres religion  hvorfor skal hun overhovedet have taletid øv jeg synes bare det er så fucking trist at hun kun får det til at handle om sin egen retfærdighedsfølelse',\n",
       " 'jeg mener du har misforstået hvad privat ejendomsret er ejendomret har ingen grænser for hvad  eller med hvem man ejer noget med pointen er at ejendom er privat og man selv kan betemme hvad man vil eje jeg tror denne ret vil blive afskaffet med din demokratiske socialisme',\n",
       " 'læs min bog eller en hvilken som helst artikel jeg har skrevet og du vil se at jeg ikke støtter regimet at jeg har brugt hele min karriere på at kritisere regimet og udstille dens undertrykkelse jeg ved godt nuancer gør ondt i din sorthvide verden men du er altså ynkelig',\n",
       " 'jeg tror du er naiv hvis du tror ren socialisme fører til noget godt jeg behøver blot at se på debatøres hadske retorik og jeg kan konstatere at hadet kun kan fører et sted hen hvis du vil afskaffe rigdom så kan du kun gøre det med totalitære midler punktum',\n",
       " 'jeg ved ikke rigtigt hvad der er ventreorienteret og højreorienteret mere det er hele debatten der er fordrejet og alle kan ikke se at kongen ikke har noget tøj på bortset fra den lille dreng det er synsbedrag af største klasse',\n",
       " 'hvad er dine kvalifikationer til at vurdere min “empiri” ligesom jeg ikke generelt går rund og gør mig klog på hvad journalister kan og ikke kan baseret på min begrænsede erfaringer hvad giver så dig selvtilliden til at mene at jeg tager fejl',\n",
       " 'du citerer “israelske og arabiske eksperter” men udtaler dig skråsikkert om iran klart jeg ikke kan vide hvad du baserer din viden om iran på når jeg tilfældigvis ikke kender til dig beklager at du blev så fornærmet ikke meningen god søndag',\n",
       " 'tak for svar men det er jo ikke årsagen jeg klandrer jeg for det er jeres manglende ux og kundeinformation det er under al kritik',\n",
       " 'jeg tror godt du forstår minimumsnormeringer er politisk længere er den ikke når du smider kortet om at nogen må give sig så er det fordi forslaget er politisk \\nmin pointe er institutioner ikke bør styres politisk de burde selv kunne finde ud af det',\n",
       " 'du er ikke den eneste vi er bare nogle der slet ikke kan tage det jeg må prøve at tænke mere over hvorfor det irriterer mig kan ikke kun være fordi jeg har en ba i persisk sprog og kultur',\n",
       " 'det er dig der retweeter mig bygger en stråmand og så nægter at forholde dig til substansen når jeg tager mig tid til at svare men det var da godt at nogen tager det alvorligt  så basalt et spørgsmål af så stor betydning utroligt du som analytiker ikke kan forstå det',\n",
       " 'du er som sagt flere gange hjertens velkommen til at unfollow jeg har heller ikke synderlig interesse i at have en follower der siger jeg støtter regimet når de absolut intet ved om mig mit virke og min forskning farvel']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performs quite allright on the test data: but slow (7 hr on whole dataset)\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "# Apply tqdm to the apply function to track progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# Use the apply function with progress tracking\n",
    "sentence_embeddings = test['preprocessed_text'].progress_apply(lambda x: get_bert_embedding(x, tokenizer, model))\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "sentence =\"jeg forstår din frustration men lad os huske den gode tone jeg finder ikke din kommentar særlig konstruktiv vi har måske forskellige synspunkter men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforstår hvad\"\n",
    "prosocial_sentence_embedding = get_bert_embedding(sentence, tokenizer, model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity between the word \"politik\" and each sentence with progress tracking\n",
    "cosine_similarities = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(sentence_embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "# Output the cosine similarities\n",
    "test['BERT_cos_sim_prosocial_sentence'] = cosine_similarities\n",
    "\n",
    "# Sort the DataFrame by cosine similarity in descending order\n",
    "sorted_replies = test.sort_values(by='BERT_cos_sim_prosocial_sentence', ascending=False)\n",
    "\n",
    "# Select the top 15 entries\n",
    "top_replies = sorted_replies.head(30)\n",
    "\n",
    "# Print the results\n",
    "#print(top_replies[['text', 'BERT_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208782c-a50d-4fca-9387-8889d3b469bc",
   "metadata": {},
   "source": [
    "#### XLM-RoBERTa (on prosical sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed0346e-a5d4-4c73-8578-c141a1dd6d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generating Embeddings: 100%|██████████████| 20000/20000 [11:35<00:00, 28.77it/s]\n",
      "Calculating Cosine Similarities: 100%|██| 20000/20000 [00:02<00:00, 8095.84it/s]\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/1634502232.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['roberta_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['almindelige mennesker har heller ikke råd til at bo alle steder jeg gad godt bo på strandvejen men det har jeg ikke råd til skal vi så til at lave en lov om det \\n\\ndesuden kan du ikke renovere lejlighed hvor du bor i forvejen  det kun når den står tom muligheden opstår',\n",
       " 'jeg stiller altid op hvornårsomhelst til debat ang iran men der er ik ngt at misforstå her jeg synes det er et meget relevant og vigtigt spm at stille om der var specifik trussel mod danske skibe før snak om maritim indsats det gør puck ikke',\n",
       " 'det er dig der retweeter mig bygger en stråmand og så nægter at forholde dig til substansen når jeg tager mig tid til at svare men det var da godt at nogen tager det alvorligt  så basalt et spørgsmål af så stor betydning utroligt du som analytiker ikke kan forstå det',\n",
       " 'jeg tror godt du forstår minimumsnormeringer er politisk længere er den ikke når du smider kortet om at nogen må give sig så er det fordi forslaget er politisk \\nmin pointe er institutioner ikke bør styres politisk de burde selv kunne finde ud af det',\n",
       " 'du misforstår hvad borgerlige vil de går ikke imod normeringer de går ind for afregulering og lade pengene følge barnet så kan institutionerne konkurrere om at levere den bedste service men det forstår  ikke',\n",
       " 'men pas nu på med at hop på at det er forslag jeg har ikke stillet de 49 forslag nogle af dem er selvfølgelig kendt som lapositioner andre er bestemt ikke men forstår godt det kan misforstås når  stiller det sådan op',\n",
       " 'men der passer jo heller ikke som det er fremgået så mener jeg at man skal sikre kongressens sikkerhed men at man normaliserer det radikaliserede ved at lade den afholde i folketinget der er ingen modsætning mellem de to synspunkter',\n",
       " 'tror du ikke vi andre er interesserede i fakta du udviser absolut ingen grad ad lydhør bare din egen altoverskyggende bedrevidenhed og komplette blinde tillid til en hel stand  i sådan en grad at du tillader dig at kalde andre løgnere',\n",
       " ' ikkeakademiske arbejdsformer men derfor kan vi stadig godt diskutere teori på et abstrakt plan med brug af komplicerede sætninger det er helt fint at kunne begge dele og lidt af en stråmand at sige at vi kun uddanner til det ene',\n",
       " 'jeg er med på at voldelig revolution ikke er del af agendaen men nu snakker du om indvandring jeg er totalt forvirret \\n\\nvenstrefløjen hader rige mennesker ja eller nej',\n",
       " 'eller trykkede på billedet og læste navnet der jeg er nok bare ret dårlig til at gemme mig men rigtig mange tak for den konstruktive råd der desværre ikke flytter fokus væk fra budskabet som det ellers var meningen',\n",
       " 'lige nu ligger dens værdi kun i at folk tror den vil stige i værdi\\npå et eller andet tidspunkt falder den nok kraftigt og stabiliserer sig så ejerne rent faktisk begynder at bruge den men indtil da er det blot en ikkevaluta',\n",
       " 'vi er ikke uenige om hvorvidt man skal sikre dem at holde et møde lad os lige holde fast i det \\njeg forstår bare ikke at man derfor skal give dem adgang til at holde kongres i folketinget jeg tror at det vil medvirke til normalisering af de ekstreme',\n",
       " 'jeg har kun set highlights men det er jo helt skørt at okore sparker kliment bagefter og smadre hans korsbånd jeg er sikker på at han ikke har intention om at skade ham men stadigvæk ikke i orden at lave sådan en hævn aktion',\n",
       " 'tænker at i lige kan vende den her i næste afsnit det er da godt nok ved at være lidt for plat hvad de har gang i over på heden jeg ville være pinlig hvis jeg var dem måske høre hvad deres fangruppe eller fanpodcast synes',\n",
       " 'desuden kryber du igen ud om det tweetet handlede om og som jeg konstant minder dig om  præmissen om at vi som princip uddanner folk til at skrive dårligt nu siger du så det var en spøg så du er altså enig i at det er forkert',\n",
       " 'tak for svar jeg er helt med på at det er en udfordring selvom odense lader til at have fået styr på det men jeg håber at i vil slå ned på selskaber der konsekvent bryder reglerne og fjerne deres tilladelse i hvert fald indtil de uddanner deres ansatte',\n",
       " 'det er der ved at blive og engagere mig kan jeg med masser af held blive hørt i partiet med tiden derudover kan jeg støtte de socialdemokratiske kræfter der stadig sidder højt i partiet jeg har for eksempel ikke tænkt mig at holde kæft på kongressen',\n",
       " 'jeg er ikke ekspert i den slags og eksperterne er så vidt jeg ved uenige det kan passe det kan være disinformation  hvem ved iranerne handler ofte ulovligt særligt når de er presset op i en krog',\n",
       " 'jeg fastholder at det der står i artiklen er en forklaring på hvorfor i reagerede som trump ikke en reel undskyldning så må du tale med journalisten hvis du mener at du er fejlciteret eller dit sidste citat er taget ud af kontekst']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decent on the test data (takes also 7hr on full dataset)\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "# Function to get RoBERTa embeddings\n",
    "def get_roberta_embedding(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "\n",
    "# Generate embeddings for DataFrame\n",
    "embeddings = [get_roberta_embedding(text, tokenizer, model) for text in tqdm(test['preprocessed_text'], desc=\"Generating Embeddings\")]\n",
    "\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "sentence = \"jeg forstår din frustration men lad os huske den gode tone jeg finder ikke din kommentar særlig konstruktiv vi har måske forskellige synspunkten men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforstår hvad\"\n",
    "prosocial_sentence_embedding = get_roberta_embedding(sentence, tokenizer, model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "test['roberta_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "\n",
    "# Sort and display top results\n",
    "sorted_df = test.sort_values(by='roberta_cos_sim_prosocial_sentence', ascending=False)\n",
    "top_replies = sorted_df.head(30)\n",
    "\n",
    "#print(top_replies[['text', 'roberta_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies['preprocessed_text'].head(20).tolist()\n",
    "text_to_print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c76df-5385-4a34-b269-df7533acf982",
   "metadata": {},
   "source": [
    "#### Fasttext (an extension of Word2Vec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9871e7da-75c3-4084-9ff9-509e31972661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|█████████| 642613/642613 [00:56<00:00, 11461.60it/s]\n",
      "Calculating Cosine Similarities: 100%|█| 642613/642613 [01:17<00:00, 8241.49it/s\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/2306015175.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['fasttext_cos_sim_hate_sentence'] = [cosine_similarity([embedding], hate_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['du er sgu endnu engang for dum til at jeg gider at bruge tid på dig',\n",
       " 'du har da ikke selv holdt dig for god til at anmelde hvad du vurderede at være en falsk reklame så måske skulle du finde en mindre hest at sidde på',\n",
       " 'nu slukker jeg sgu for dig for du er virkelig irriterende at høre på',\n",
       " 'nu er du bare dum at høre på her troede man at man kunne have en alm diskussion med en voksen mand trist',\n",
       " 'shit da en røvlort \\njeg er fandeme ked af at du har set dig nødsaget til at gøre dette og gal for helvede men jeg håber det betyder at du får en bedre oplevelse her\\nsikke en lortestodder',\n",
       " 'at du sender mig en tanke når du ser en smuk pige spille fiol har jeg intet ondt at sige om \\ngodmorgen',\n",
       " 'håber du er okay lad mig vide hvis du skal bruge en skulder at græde på',\n",
       " 'du er fanme da for dum at høre på jeg håber da at ulven tager sig kærligt af dig og dine så får piben en anden lyd',\n",
       " 'jeg tror du tvivler på alt fordi du synes den her kamp er en du lige skal blande dig i uden at tænke på andre tak for solidariteten kammerat',\n",
       " 'stop dig selv kathe gider ikke at høre på forsvar for så latterlig en kommentar',\n",
       " 'hold nukæft hvor er du dum at høre på  højrefløjs trolls som dig fortjener ikke svar',\n",
       " 'du formår fint at vide at du ikke har fattet en skid en moderne lad dem spise kage idioti',\n",
       " 'ja for du rammer aldrig forkert på tasterne det er en joke din klump undskyld jeg fornærmede din religion btw vi snakkede om at se fodbold du ved at det ikke giver motion ikke 😉',\n",
       " 'og du er en iriterende dum gås drop at diskutere med mig hvis du ikke kan være seriøs',\n",
       " 'shit du er en landsbytosse at høre på hjem under stenen med dig',\n",
       " 'arghhhhh træls at du kalder det udskamning at jeg giver udtryk for en anden holdning',\n",
       " 'hahaha du er en kvinde af få ord du forventer at se dig på vores arrangement  har du været på arendalsuka før for så forventer jeg mig at nordmanden i dig kan guide mig lidt ',\n",
       " 'for mig eller dig du kan heldigvis vælge at skippe mine tweets  jeg havde foretrukket at læse bogen på en ferie',\n",
       " 'jaaa mand ønsker mig kun at du breaker svaret en fredag aften ',\n",
       " 'du siger jeg har lige googlet dig det forklarer en del efterfulgt af jeg dømmer dig ikke 😅er da glad for at du kan se din måde at debattere på  som jeg har parodieret  er stærkt barnlig og uvirksom ift at få din pointe frem og altså ret komisk \\nhttpstcok7zexxahch']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hate sentence embeddings, on whole dataset\n",
    "\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the pretrained FastText model: takes 5 min to load: uncomment after first run\n",
    "fasttext_model = load_facebook_model('/Users/idahelenedencker/Desktop/STANDBY_Ida/Creating dataset of reference tweets/Models/cc.da.300.bin')\n",
    "\n",
    "# Function to get FastText embeddings\n",
    "def get_fasttext_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_embeddings = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not word_embeddings:  # If no words in the sentence are in the vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Generate embeddings for DataFrame\n",
    "embeddings = [get_fasttext_embedding(text, fasttext_model) for text in tqdm(replies['preprocessed_text'], desc=\"Generating Embeddings\")]\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "hate_sentence = \"du for dum at høre på sikke en idiot jeg hader dig\"\n",
    "hate_sentence_embedding = get_fasttext_embedding(hate_sentence, fasttext_model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "replies['fasttext_cos_sim_hate_sentence'] = [cosine_similarity([embedding], hate_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "# Sort and display top results\n",
    "sorted_df = replies.sort_values(by='fasttext_cos_sim_hate_sentence', ascending=False)\n",
    "top_replies_hate = sorted_df.head(4000)\n",
    "\n",
    "#print(top_15_replies[['text', 'fasttext_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_hate['preprocessed_text'].head(20).tolist()\n",
    "text_to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04eadd5e-c5be-4059-b7d3-19e5dc1cbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|█████████| 642613/642613 [00:51<00:00, 12401.56it/s]\n",
      "Calculating Cosine Similarities: 100%|█| 642613/642613 [01:16<00:00, 8429.34it/s\n",
      "/var/folders/ds/0d9wxy210kx_fvknqn3hcg_h0000gn/T/ipykernel_6347/449282597.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  replies['fasttext_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jeg siger det vel strengt taget bare til dem der følger mig  men hvis du følger den efterfølgende diskussion så er jeg netop ikke imponeret over den tilgang spillerne har',\n",
       " 'had alt det du vil men du skal ikke gøre dig til dommer over hvad jeg mener er en naturlig reaktion jeg dækker overhovedet ikke den sag',\n",
       " 'din første indvending er muligvis korrekt det skal vurderes den anden forstår jeg ikke hvilke andre vacciner tilbyder vi offlabel',\n",
       " 'hvis det ovenikøbet er er kendt adfærd så fortsætter den jo udelukkende fordi ingen tager den videre  han kan jo desværre ikke se det  hvis du ikke ønsker den store sag  så tror jeg du skal gøre mere ved selve konfrontationen med ham  giv din vrede meregas  for din skyld ♥️',\n",
       " 'det skrev jeg ikke men uanset hvad jeg kaster efter dig vil du jo ikke godtage det  jeg har ikke en blind tro men jeg vurdere efter bedste evne at man har gjort det rigtige med fejl og alt muligt undervejs klart men grundlæggende den rigtige respons',\n",
       " 'tror det er den gamle lortesag mange refererer til og det skal man ikke joke med spørger du mig ',\n",
       " 'okay trine skal vi åbne for dine spørgsmål til tenet jeg mener faktisk ikke den er så uigennemskuelig endda',\n",
       " 'det er jeg ked af jeg vil gerne forstå din bekymring  det er derfor jeg svarer dig  men jeg har desværre lidt svært ved at følge dit ræsonnement der lyder som om vi snakker forbi hinanden måske er det twitter jeg ved det ikke men tager altid gerne en seriøs dialog',\n",
       " 'lidt men jeg vil faktisk rigtig gerne forstå din kritik  vi rammer ikke bredt nok skriver du hvordan',\n",
       " 'vi opfatter nok bare debatten forskelligt jeg synes blot du understreger min pointe selvom det næppe er din hensigt 🤷🏻\\u200d♀️',\n",
       " 'skal jeg tage din med når jeg er dernede jeg respekterer du er uenig med mig men ikke din tone det er unødvendigt',\n",
       " 'jamen jeg forstår dine holdninger på området qua din tilknytning og det er absolut fair nok 😉\\n\\nmen demokratisk vedtagne love skal ikke følges hvis vi bare føler noget andet \\n\\ner d den form for politiker du vil være hvis du opnår mandat\\n\\nbegge dele skrevet med respekt',\n",
       " 'selvfølgelig kan jeg det men jeg vil dog ikke lade det afspore samtalen fra substansen som jeg ser du gør her selvom din intention er udemærket jeg tager afstand fra racistiske udtalelser og ideer og ønsker ikke at sugarcoate dem med at vi er gode mennesker alligevel',\n",
       " 'næææ det vidste jeg ikke det jeg siger er bare at lovlig vigtigt emne til dén type twitterskænderier er nok heller ikke den direkte vej til mere sammenhængsamarbejde og svaret på dit spørgsmål har vi vel iøvrigt forlængst givet 🤔',\n",
       " 'fair pointe konspirationsteori er nok til den gode side😂 jeg er ikke ekspert i hvordan og hvor længe teledata opbevares inden det overskrives måske du kan gøre os klogere her',\n",
       " 'tak for diskussionen nikolaj vi kommer det ikke videre jeg har skrevet hvad jeg mener men jeg er faktisk stadig ikke helt klar over hvad du mener jeg har gjort forkert  og hvad dit argumentbevæggrund for forklare patriots ageren i sagen er',\n",
       " 'seriøs journalistik enig påstanden er jo bare vanvittig eftersom det netop er en påstand blandt alt for mange andre godt den infantile adfærd udfordres men lad os bære over har læst at hun kan lide brøndby',\n",
       " 'jeg aner ikke hvad din drøm er men jeg håber du gør dem alle til skamme ',\n",
       " 'det opsummerer det hele godt kald os bare gamle sure mænd jeg vil kalde det rettidigt omhu håber inderligt du får ret men jeg ville kalde vores sportslige ledelse decideret naiv hvis de planlægger på den slags forhåbninger der er alt for stor sandsynlighed for det går galt',\n",
       " 'den er ikke forkert hvis vi bare holder os til det fodboldmæssige 😀 jeg tror det ville være godt for alle hvis han snart blev skudt afsted']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prosocial sentence embeddings, on whole dataset\n",
    "\n",
    "\n",
    "# Function to get FastText embeddings\n",
    "def get_fasttext_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_embeddings = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not word_embeddings:  # If no words in the sentence are in the vocabulary\n",
    "        return np.zeros(model.vector_size)\n",
    "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Generate embeddings for DataFrame\n",
    "embeddings = [get_fasttext_embedding(text, fasttext_model) for text in tqdm(replies['preprocessed_text'], desc=\"Generating Embeddings\")]\n",
    "\n",
    "# Get embedding for hate sentence\n",
    "prosocial_sentence = \"jeg forstår din frustration men lad os huske den gode tone jeg finder ikke din kommentar særlig konstruktiv vi har måske forskellige synspunkten men had bidrager ikke til den konstruktive debat dette er ikke sandt det er faktuelt forkert du misforstår hvad\"\n",
    "prosocial_sentence_embedding = get_fasttext_embedding(prosocial_sentence, fasttext_model).reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "replies['fasttext_cos_sim_prosocial_sentence'] = [cosine_similarity([embedding], prosocial_sentence_embedding)[0][0] for embedding in tqdm(embeddings, desc=\"Calculating Cosine Similarities\")]\n",
    "\n",
    "# Sort and display top results\n",
    "sorted_df = replies.sort_values(by='fasttext_cos_sim_prosocial_sentence', ascending=False)\n",
    "top_replies_prosocial = sorted_df.head(4000)\n",
    "\n",
    "#print(top_15_replies[['text', 'fasttext_cos_sim_hate_sentence']])\n",
    "\n",
    "#print the whole text\n",
    "text_to_print = top_replies_prosocial['preprocessed_text'].head(20).tolist()\n",
    "text_to_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82057bb1-34cc-4a02-af53-e10d71099931",
   "metadata": {},
   "source": [
    "### Using Fasttext embeddings (20 k sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "111c666b-e047-4f32-b92f-4ae41430254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 8000/8000/4000 split: using fasttext\n",
    "\n",
    "#delete tfidf_cos_sim_hate_sentence and tfidf_cos_sim_prosocial_sentence columns\n",
    "replies = replies.drop(['tfidf_cos_sim_hate_sentence', 'tfidf_cos_sim_prosocial_sentence'], axis=1)\n",
    "\n",
    "#make dataframes based on cosine similarity scores \n",
    "sorted_df_prosocial = replies.sort_values(by='fasttext_cos_sim_prosocial_sentence', ascending=False).head(8000)\n",
    "sorted_df_hate = replies.sort_values(by='fasttext_cos_sim_hate_sentence', ascending=False).head(8000)\n",
    "\n",
    "#make random dataframe\n",
    "sorted_df_random = replies.sample(4000)\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "word_embeddings_fasttext = pd.concat([sorted_df_prosocial, sorted_df_hate,sorted_df_random], axis=0)\n",
    "word_embeddings_fasttext\n",
    "\n",
    "#save\n",
    "word_embeddings_fasttext.to_csv('/Users/idahelenedencker/Desktop/STANDBY_Ida/Creating dataset of reference tweets/replies_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
