{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5835ba-dffa-471f-b0d8-d81f0fe3027c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.44.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# packages needed (listed in requirenments file)\n",
    "\n",
    "!pip install matplotlib numpy pandas seaborn scikit-learn torch tqdm transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f750c674-4b26-425f-96eb-03b2154c8000",
   "metadata": {},
   "source": [
    "Info of data: \n",
    "\n",
    "Train csv: 5184 rows (80%)\n",
    "Test csv: 1296 rows (20%)\n",
    "6480 rows in total\n",
    "\n",
    "When testing the script reduce sample: \n",
    "5184/6 = 864\n",
    "1296/6 = 216 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f82d9-d1ad-4ecf-addb-dc221a7af71d",
   "metadata": {},
   "source": [
    "# Model without reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e3a9ac3-b5fd-4860-b6d0-b1524df72dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works, dont edit!\n",
    "# Retrain the 01_model_without_reference with the original model\n",
    "\n",
    "\"\"\"\n",
    "==============================\n",
    "Model Training - No Reference\n",
    "==============================\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Set Up -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Parameters -- \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Change model here if desired\n",
    "    batch_size = 16 # Small batch number for processing, can increase if using GPU\n",
    "    num_labels = 12\n",
    "\n",
    "    # Load Data \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_no_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_no_ref.csv\").head(216)\n",
    "\n",
    "    # -- Classes -- \n",
    "    class TextDataset(Dataset):\n",
    "        # initialise the text, labels, the chosen tokenizer (BERT) and set the maximum length to BERT's max (512)\n",
    "        def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        # Return the total number of instances in the dataset (i.e., the number of rows)\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        # Take the text and labels as indexes, and return a dictonary of the tokenised text and its corresponding label \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            encodings = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\"input_ids\": encodings[\"input_ids\"][0], \"attention_mask\": encodings[\"attention_mask\"][0], \"labels\": torch.tensor(label, dtype=torch.float)}\n",
    "\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Model -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Define labels -- \n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "                                   \n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- Create datasets -- \n",
    "    train_dataset = TextDataset(train_data['rep_text'].tolist(), train_labels, tokenizer, max_length=512)\n",
    "    test_dataset = TextDataset(test_data['rep_text'].tolist(), test_labels, tokenizer, max_length=512)\n",
    "\n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # -- Set the optimizer, loss function and learning rate -- \n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # -- Training model -- \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- Evaluate Function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = (logits.sigmoid() > 0.5).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "    \"\"\"\n",
    "    ======================\n",
    "    ----- Test model -----\n",
    "    ======================\n",
    "    \"\"\"\n",
    "    #  -- Call device --\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -- Setup saving parameters -- \n",
    "    num_epochs = 1\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference/\"\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # Define a path to save the model checkpoints\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference/checkpoints/\"\n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e17b0-fb00-4a3a-bf76-968212b3bed8",
   "metadata": {},
   "source": [
    "Note to self: there are 5184 texts in the training data: train_data_no_ref.csv, and depeding on the batch size, the number of batches pr epoch (number displayed in the tqdm when training the model) will change (e.g. 16 batches will be 324 because 5184/16= 324)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c249fa06-9612-44d3-82ac-c855351484b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 54/54 [18:40<00:00, 20.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5637614219276993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 14/14 [00:43<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4918421357870102\n",
      "  Train Loss = 0.5638\n",
      "  Test Loss = 0.4918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2294e113-0734-48fc-859e-ae0d7cfc8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 01_model_without_reference with a new model (XLM-Roberta)\n",
    "# looks to be working, need to fully run\n",
    "\n",
    "\"\"\"\n",
    "==============================\n",
    "Model Training - No Reference\n",
    "==============================\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "#new\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def main_2():\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Set Up -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Parameters -- \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large\") # Change model here if desired\n",
    "    batch_size = 16 # Small batch number for processing, can increase if using GPU\n",
    "    num_labels = 12\n",
    "\n",
    "    # Load Data \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_no_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_no_ref.csv\").head(216)\n",
    "\n",
    "    # -- Classes -- \n",
    "    class TextDataset(Dataset):\n",
    "        # initialise the text, labels, the chosen tokenizer (BERT) and set the maximum length to BERT's max (512)\n",
    "        def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        # Return the total number of instances in the dataset (i.e., the number of rows)\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        # Take the text and labels as indexes, and return a dictonary of the tokenised text and its corresponding label \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            encodings = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\"input_ids\": encodings[\"input_ids\"][0], \"attention_mask\": encodings[\"attention_mask\"][0], \"labels\": torch.tensor(label, dtype=torch.float)}\n",
    "\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Model -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Define labels -- \n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "                                   \n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- Create datasets -- \n",
    "    train_dataset = TextDataset(train_data['rep_text'].tolist(), train_labels, tokenizer, max_length=512)\n",
    "    test_dataset = TextDataset(test_data['rep_text'].tolist(), test_labels, tokenizer, max_length=512)\n",
    "\n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # -- Set the optimizer, loss function and learning rate -- \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-large\", num_labels=num_labels)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # -- Training model -- \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- Evaluate Function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = (logits.sigmoid() > 0.5).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "    \"\"\"\n",
    "    ======================\n",
    "    ----- Test model -----\n",
    "    ======================\n",
    "    \"\"\"\n",
    "    #  -- Call device --\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -- Setup saving parameters -- \n",
    "    num_epochs = 1\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference_new/\"\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # Define a path to save the model checkpoints\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference_new/checkpoints/\"\n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062ba0ed-374d-4c3f-8a7c-94d66fa9255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|████████                       | 14/54 [23:31<1:04:58, 97.46s/it][E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "Training:  26%|███████▊                      | 14/54 [25:07<1:11:46, 107.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 195\u001b[0m, in \u001b[0;36mmain_2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# -- Train model -- \u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Save model checkpoint\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m, in \u001b[0;36mmain_2.<locals>.train\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    124\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m    125\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 126\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    130\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adamw.py:413\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    412\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 413\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    416\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08352b5-65c0-4a77-bd19-c06647d5da05",
   "metadata": {},
   "source": [
    "# Model with reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96069172-37fe-43a0-8525-eb663afcdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works, dont edit\n",
    "# Retrain the 02_model_with_ref with the original model\n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "Model Training 2 - Reference Text\n",
    "=================================\n",
    "\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "To include the reference tweet information, there is one modification to this model from the basic 'No Reference Model' (01_model_no_ref.py): \n",
    "1. The reference text is appended to the reply tweet text with a [SEP] token to delineate the two tweets before the classification is made.\n",
    "    - This means the model has more information to base its prediction upon as it can see patterns in the reference text and how this may influence the reply tweet text. \n",
    "\n",
    "Usage:\n",
    "  $ python3 src/02_model_with_ref.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "----- Import Depenendencies -----\n",
    "=================================\n",
    "\"\"\"\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "=========================\n",
    "----- Main Function -----\n",
    "=========================\n",
    "\"\"\"\n",
    "def main_3():\n",
    "    \"\"\"\n",
    "    ==========================\n",
    "    Parameters and Directories \n",
    "    ==========================\n",
    "    \"\"\"\n",
    "    # -- Model parameters -- \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Can change BERT model\n",
    "    batch_size = 16     # small batch size due to computation, increase to 32 if using GPU\n",
    "    num_labels = 12\n",
    "    num_epochs = 1      # Increase if desired, model started to overfit after around 5 epochs\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # -- Directories --\n",
    "    #input_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data\"\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text/\"\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text/checkpoints/\" # path for checkpoints to be saved\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    =======\n",
    "    Classes\n",
    "    =======\n",
    "    \"\"\"\n",
    "    # create a Dataset class for text and reference labels\n",
    "    class TextDatasetWithRefLabels(Dataset):\n",
    "        def __init__(self, ref_texts, ref_labels, texts, labels, tokenizer, max_length):\n",
    "            self.ref_texts = ref_texts\n",
    "            self.ref_labels = ref_labels\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Binarize the reference labels\n",
    "            self.mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "            self.ref_labels = self.mlb.fit_transform(ref_labels)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            ref_text = self.ref_texts[index]\n",
    "            ref_label = self.ref_labels[index]\n",
    "            text = self.texts[index]\n",
    "            label = self.labels[index]\n",
    "\n",
    "            # Tokenize the reply tweet \n",
    "            rep_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # Determine the maximum length of the reference text that can be used \n",
    "            max_ref_length = 511 - len(rep_tokens)\n",
    "\n",
    "            # Tokenize the reference text and truncate if necessary\n",
    "            ref_tokens = self.tokenizer.tokenize(ref_text)\n",
    "            if len(ref_tokens) > max_ref_length:\n",
    "                truncated_ref_tokens = ref_tokens[-max_ref_length:]\n",
    "            else:\n",
    "                truncated_ref_tokens = ref_tokens\n",
    "            \n",
    "            # Combine the reference and reply text tokens with a [SEP] token\n",
    "            combined_tokens = truncated_ref_tokens + [self.tokenizer.sep_token] + rep_tokens \n",
    "\n",
    "            # Convert the combined tokens to a text string\n",
    "            combined_text = self.tokenizer.convert_tokens_to_string(combined_tokens)\n",
    "\n",
    "            # Encode the combined text\n",
    "            encoded_data = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation='only_first',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_data['input_ids']\n",
    "            attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "            # Convert the label data to tensors\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            ref_label_tensor = torch.tensor(ref_label, dtype=torch.float32)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'attention_mask': attention_mask.squeeze(0),\n",
    "                'ref_labels': ref_label_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "\n",
    "    class BertForContextualClassification(nn.Module):\n",
    "        def __init__(self, num_labels):\n",
    "            super(BertForContextualClassification, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.linear = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, ref_input_ids=None, ref_attention_mask=None):\n",
    "            if ref_input_ids is not None:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            else:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            pooled_output = output[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.linear(pooled_output)\n",
    "            return logits\n",
    "\n",
    "    \"\"\"\n",
    "    =============\n",
    "    Preprocessing\n",
    "    =============\n",
    "    \"\"\"\n",
    "    # -- Load Data -- \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_ref.csv\").head(216)\n",
    "\n",
    "    # prepare the train and test labels using MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "\n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- create train and test datasets -- \n",
    "    train_dataset = TextDatasetWithRefLabels(train_data['ref_text'].tolist(),\n",
    "                                            train_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            train_data['rep_text'].tolist(),\n",
    "                                            train_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "\n",
    "    test_dataset = TextDatasetWithRefLabels(test_data['ref_text'].tolist(),\n",
    "                                            test_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            test_data['rep_text'].tolist(),\n",
    "                                            test_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "    \n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \"\"\"\n",
    "    =====\n",
    "    Model\n",
    "    =====\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = BertForContextualClassification(num_labels)\n",
    "\n",
    "    # Define your optimizer, loss function and learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Create training function \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            ref_labels = batch[\"ref_labels\"].to(device)\n",
    "            #print(\"input_ids shape:\", input_ids.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- create evaluation function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).cpu().detach().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "    \n",
    "    # -- Wrap it up into a function -- \n",
    "    # Call device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # Evaluate the training data \n",
    "        #train_loss, true_labels, pred_labels = evaluate(model, train_dataloader, criterion, device)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "        # Activate if kernel struggles to run model \n",
    "        #if epoch < 5:\n",
    "            #del train_loss, true_labels, pred_labels, test_loss, test_true_labels, test_pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad6320a-ff91-4cdb-8682-96a11f46c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 54/54 [18:46<00:00, 20.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.548078970224769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 14/14 [02:38<00:00, 11.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.46460294297763277\n",
      "  Train Loss = 0.5481\n",
      "  Test Loss = 0.4646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba43483-8205-415d-a55c-53defec1ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 02_model_with_ref with a new model (XLM-Roberta)\n",
    "# looks to be working, need to fully run\n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "Model Training 2 - Reference Text\n",
    "=================================\n",
    "\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "To include the reference tweet information, there is one modification to this model from the basic 'No Reference Model' (01_model_no_ref.py): \n",
    "1. The reference text is appended to the reply tweet text with a [SEP] token to delineate the two tweets before the classification is made.\n",
    "    - This means the model has more information to base its prediction upon as it can see patterns in the reference text and how this may influence the reply tweet text. \n",
    "\n",
    "Usage:\n",
    "  $ python3 src/02_model_with_ref.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "----- Import Depenendencies -----\n",
    "=================================\n",
    "\"\"\"\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# new\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "from transformers import BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "=========================\n",
    "----- Main Function -----\n",
    "=========================\n",
    "\"\"\"\n",
    "\n",
    "def main_4():\n",
    "    \"\"\"\n",
    "    ==========================\n",
    "    Parameters and Directories \n",
    "    ==========================\n",
    "    \"\"\"\n",
    "    # -- Model parameters -- \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large\") # Can change BERT model\n",
    "    batch_size = 16     # small batch size due to computation, increase to 32 if using GPU\n",
    "    num_labels = 12\n",
    "    num_epochs = 1      # Increase if desired, model started to overfit after around 5 epochs\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # -- Directories --\n",
    "    #input_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data\"\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_new/\"\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_new/checkpoints/\" # path for checkpoints to be saved\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    =======\n",
    "    Classes\n",
    "    =======\n",
    "    \"\"\"\n",
    "    # create a Dataset class for text and reference labels\n",
    "    class TextDatasetWithRefLabels(Dataset):\n",
    "        def __init__(self, ref_texts, ref_labels, texts, labels, tokenizer, max_length):\n",
    "            self.ref_texts = ref_texts\n",
    "            self.ref_labels = ref_labels\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Binarize the reference labels\n",
    "            self.mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "            self.ref_labels = self.mlb.fit_transform(ref_labels)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            ref_text = self.ref_texts[index]\n",
    "            ref_label = self.ref_labels[index]\n",
    "            text = self.texts[index]\n",
    "            label = self.labels[index]\n",
    "\n",
    "            # Tokenize the reply tweet \n",
    "            rep_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # Determine the maximum length of the reference text that can be used \n",
    "            max_ref_length = 511 - len(rep_tokens)\n",
    "\n",
    "            # Tokenize the reference text and truncate if necessary\n",
    "            ref_tokens = self.tokenizer.tokenize(ref_text)\n",
    "            if len(ref_tokens) > max_ref_length:\n",
    "                truncated_ref_tokens = ref_tokens[-max_ref_length:]\n",
    "            else:\n",
    "                truncated_ref_tokens = ref_tokens\n",
    "            \n",
    "            # Combine the reference and reply text tokens with a [SEP] token\n",
    "            combined_tokens = truncated_ref_tokens + [self.tokenizer.sep_token] + rep_tokens \n",
    "\n",
    "            # Convert the combined tokens to a text string\n",
    "            combined_text = self.tokenizer.convert_tokens_to_string(combined_tokens)\n",
    "\n",
    "            # Encode the combined text\n",
    "            encoded_data = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation='only_first',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_data['input_ids']\n",
    "            attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "            # Convert the label data to tensors\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            ref_label_tensor = torch.tensor(ref_label, dtype=torch.float32)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'attention_mask': attention_mask.squeeze(0),\n",
    "                'ref_labels': ref_label_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "\n",
    "    class BertForContextualClassification(nn.Module):\n",
    "        def __init__(self, num_labels):\n",
    "            super(BertForContextualClassification, self).__init__()\n",
    "            self.bert = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.linear = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, ref_input_ids=None, ref_attention_mask=None):\n",
    "            if ref_input_ids is not None:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            else:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            pooled_output = output[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.linear(pooled_output)\n",
    "            return logits\n",
    "\n",
    "    \"\"\"\n",
    "    =============\n",
    "    Preprocessing\n",
    "    =============\n",
    "    \"\"\"\n",
    "    # -- Load Data -- \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_ref.csv\").head(216)\n",
    "\n",
    "    # prepare the train and test labels using MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "\n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- create train and test datasets -- \n",
    "    train_dataset = TextDatasetWithRefLabels(train_data['ref_text'].tolist(),\n",
    "                                            train_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            train_data['rep_text'].tolist(),\n",
    "                                            train_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "\n",
    "    test_dataset = TextDatasetWithRefLabels(test_data['ref_text'].tolist(),\n",
    "                                            test_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            test_data['rep_text'].tolist(),\n",
    "                                            test_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "    \n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \"\"\"\n",
    "    =====\n",
    "    Model\n",
    "    =====\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = BertForContextualClassification(num_labels)\n",
    "\n",
    "    # Define your optimizer, loss function and learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Create training function \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            ref_labels = batch[\"ref_labels\"].to(device)\n",
    "            #print(\"input_ids shape:\", input_ids.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- create evaluation function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).cpu().detach().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "    \n",
    "    # -- Wrap it up into a function -- \n",
    "    # Call device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # Evaluate the training data \n",
    "        #train_loss, true_labels, pred_labels = evaluate(model, train_dataloader, criterion, device)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "        # Activate if kernel struggles to run model \n",
    "        #if epoch < 5:\n",
    "            #del train_loss, true_labels, pred_labels, test_loss, test_true_labels, test_pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37b531-3f47-4a66-ae7a-d57cc4330a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97f8ff0f968447db0730f7f01ff8f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c315dc6379ef41a6b76d0f45a882be0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▌                               | 1/54 [01:27<1:17:27, 87.69s/it]"
     ]
    }
   ],
   "source": [
    "main_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed47ea4-1dfb-43cf-8445-e54ffb836fd0",
   "metadata": {},
   "source": [
    "# Model with reference and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e892ea1-1d24-4567-a20b-99eba2b08645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test?\n",
    "# Retrain the 03_model_with_ref_and_labels with the original model\n",
    "\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# new\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from transformers import XLMRobertaModel\n",
    "\n",
    "from transformers import BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def main_5():\n",
    "    \"\"\"\n",
    "    ==========================\n",
    "    Parameters and Directories \n",
    "    ==========================\n",
    "    \"\"\"\n",
    "    # Argparse parameters \n",
    "    model_name = 'bert-base-uncased'\n",
    "    num_epochs = 1\n",
    "\n",
    "    # Model parameters \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    batch_size = 16    # Increase to 32 if using on GPU for faster training\n",
    "    num_ref_labels = 12\n",
    "    num_labels = 12\n",
    "\n",
    "    # Directories \n",
    "    #input_dir = \"./data/\"\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_and_labels/\"\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_and_labels/checkpoints/\" # path for checkpoints to be saved\n",
    "    \n",
    "    \"\"\"\n",
    "    =======\n",
    "    Classes\n",
    "    =======\n",
    "    \"\"\"\n",
    "    # -- Class for creating dataset with referemce text and labels -- \n",
    "    class TextDatasetWithRefLabels(Dataset):\n",
    "        def __init__(self, ref_texts, ref_labels, texts, labels, tokenizer, max_length):\n",
    "            self.ref_texts = ref_texts\n",
    "            self.ref_labels = ref_labels\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Binarize the reference labels\n",
    "            self.mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "            self.ref_labels = self.mlb.fit_transform(ref_labels)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            ref_text = self.ref_texts[index]\n",
    "            ref_label = self.ref_labels[index]\n",
    "            text = self.texts[index]\n",
    "            label = self.labels[index]\n",
    "\n",
    "            # Tokenize the reply tweet \n",
    "            rep_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # Determine the maximum length of the reference text that can be used \n",
    "            max_ref_length = 511 - len(rep_tokens)\n",
    "\n",
    "            # Tokenize the reference text and truncate if necessary\n",
    "            ref_tokens = self.tokenizer.tokenize(ref_text)\n",
    "            if len(ref_tokens) > max_ref_length:\n",
    "                truncated_ref_tokens = ref_tokens[-max_ref_length:]\n",
    "            else:\n",
    "                truncated_ref_tokens = ref_tokens\n",
    "                \n",
    "            # Combine the reference and reply text tokens with a [SEP] token\n",
    "            combined_tokens = truncated_ref_tokens + [self.tokenizer.sep_token] + rep_tokens \n",
    "\n",
    "            # Convert the combined tokens to a text string\n",
    "            combined_text = self.tokenizer.convert_tokens_to_string(combined_tokens)\n",
    "\n",
    "            # Encode the combined text\n",
    "            encoded_data = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation='only_first',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_data['input_ids']\n",
    "            attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "            # Convert the label data to tensors\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            ref_label_tensor = torch.tensor(ref_label, dtype=torch.float32)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'attention_mask': attention_mask.squeeze(0),\n",
    "                'ref_labels': ref_label_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "    \n",
    "    # -- class for modelling with reference text and labels -- \n",
    "    class BertForContextualClassification(nn.Module):\n",
    "        def __init__(self, num_labels, num_ref_labels, model_name):\n",
    "            super(BertForContextualClassification, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.linear_ref_labels = nn.Linear(num_ref_labels, num_ref_labels) # Additional layer for reference labels\n",
    "            self.linear_combined = nn.Linear(self.bert.config.hidden_size + num_ref_labels, num_labels) # Combining BERT output and reference labels\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, ref_labels):\n",
    "            # Obtain BERT's output for the combined text\n",
    "            output = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            pooled_output = output[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            # Pass the reference labels through a linear layer\n",
    "            ref_labels_output = self.linear_ref_labels(ref_labels)\n",
    "\n",
    "            # Concatenate the BERT pooled output with the processed reference labels\n",
    "            combined_output = torch.cat((pooled_output, ref_labels_output), dim=1)\n",
    "\n",
    "            # Pass the combined output through the final linear layer\n",
    "            logits = self.linear_combined(combined_output)\n",
    "\n",
    "            return logits\n",
    "\n",
    "    \"\"\"\n",
    "    =============\n",
    "    Preprocessing\n",
    "    =============\n",
    "    \"\"\"\n",
    "    # -- Load Data -- \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_ref.csv\").head(216)\n",
    "\n",
    "    #Define the labels to be binarised and transform\n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # create datasets\n",
    "    train_dataset = TextDatasetWithRefLabels(ref_texts=train_data['ref_text'].tolist(),\n",
    "                                          ref_labels=train_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                          texts=train_data['rep_text'].tolist(),\n",
    "                                          labels=train_labels,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          max_length=512)\n",
    "    \n",
    "    test_dataset = TextDatasetWithRefLabels(ref_texts=test_data['ref_text'].tolist(),\n",
    "                                         ref_labels=test_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                         texts=test_data['rep_text'].tolist(),\n",
    "                                         labels=test_labels,\n",
    "                                         tokenizer=tokenizer,\n",
    "                                         max_length=512)\n",
    "\n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \"\"\"\n",
    "    =====\n",
    "    Model\n",
    "    =====\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = BertForContextualClassification(num_labels, num_ref_labels, model_name)\n",
    "\n",
    "    # Define your optimizer, loss function and learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Create training function \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            ref_labels = batch[\"ref_labels\"].to(device) # Extract reference labels from the batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask, ref_labels=ref_labels) # Include reference labels when calling the model\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- create evaluation function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            ref_labels = batch[\"ref_labels\"].to(device) # Extract reference labels from the batch\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask, ref_labels=ref_labels) # Include reference labels when calling the model\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).cpu().detach().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "    # Call device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -- Wrap it up into a function -- \n",
    "    epoch_no = 0 \n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b88461-9d06-43e5-a706-9724b1022e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|█▎                                | 2/54 [00:46<20:01, 23.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_5\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 273\u001b[0m, in \u001b[0;36mmain_5\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# -- Train model -- \u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Save model checkpoint\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 213\u001b[0m, in \u001b[0;36mmain_5.<locals>.train\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    210\u001b[0m ref_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mref_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Extract reference labels from the batch\u001b[39;00m\n\u001b[1;32m    212\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 213\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_labels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Include reference labels when calling the model\u001b[39;00m\n\u001b[1;32m    214\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m    215\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 137\u001b[0m, in \u001b[0;36mmain_5.<locals>.BertForContextualClassification.forward\u001b[0;34m(self, input_ids, attention_mask, ref_labels)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, ref_labels):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Obtain BERT's output for the combined text\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    142\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    524\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:337\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    333\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_5()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf6304f-5655-49cb-af01-f274cb402418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 03_model_with_ref_and_labels with a new model (XLM-Roberta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
