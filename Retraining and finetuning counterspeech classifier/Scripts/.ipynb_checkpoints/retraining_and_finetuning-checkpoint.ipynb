{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393d32d0-fe3e-4d6b-b732-dcff41e8d03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/idahelenedencker/myenv/bin/python3.12\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5835ba-dffa-471f-b0d8-d81f0fe3027c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: pandas in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: torch in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: tqdm in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: transformers in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/idahelenedencker/myenv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# packages needed (listed in requirenments file)\n",
    "\n",
    "!pip3 install matplotlib numpy pandas seaborn scikit-learn torch tqdm transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f750c674-4b26-425f-96eb-03b2154c8000",
   "metadata": {},
   "source": [
    "Info of data: \n",
    "\n",
    "Train csv: 5184 rows (80%)\n",
    "Test csv: 1296 rows (20%)\n",
    "6480 rows in total\n",
    "\n",
    "When testing the script reduce sample: \n",
    "5184/6 = 864\n",
    "1296/6 = 216 \n",
    "\n",
    "Further tesing on main_4 and main_6 that crashes:\n",
    "5184/18=288\n",
    "1296/18= 72\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f82d9-d1ad-4ecf-addb-dc221a7af71d",
   "metadata": {},
   "source": [
    "# Model without reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3a9ac3-b5fd-4860-b6d0-b1524df72dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 01_model_without_reference with the original model \n",
    "# Current code is working \n",
    "\n",
    "\"\"\"\n",
    "==============================\n",
    "Model Training - No Reference\n",
    "==============================\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Set Up -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Parameters -- \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Change model here if desired\n",
    "    batch_size = 16 # Small batch number for processing, can increase if using GPU\n",
    "    num_labels = 12\n",
    "\n",
    "    # Load Data \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_no_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_no_ref.csv\").head(216)\n",
    "\n",
    "    # -- Classes -- \n",
    "    class TextDataset(Dataset):\n",
    "        # initialise the text, labels, the chosen tokenizer (BERT) and set the maximum length to BERT's max (512)\n",
    "        def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        # Return the total number of instances in the dataset (i.e., the number of rows)\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        # Take the text and labels as indexes, and return a dictonary of the tokenised text and its corresponding label \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            encodings = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\"input_ids\": encodings[\"input_ids\"][0], \"attention_mask\": encodings[\"attention_mask\"][0], \"labels\": torch.tensor(label, dtype=torch.float)}\n",
    "\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Model -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Define labels -- \n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "                                   \n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- Create datasets -- \n",
    "    train_dataset = TextDataset(train_data['rep_text'].tolist(), train_labels, tokenizer, max_length=512)\n",
    "    test_dataset = TextDataset(test_data['rep_text'].tolist(), test_labels, tokenizer, max_length=512)\n",
    "\n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # -- Set the optimizer, loss function and learning rate -- \n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # -- Training model -- \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- Evaluate Function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = (logits.sigmoid() > 0.5).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "    \"\"\"\n",
    "    ======================\n",
    "    ----- Test model -----\n",
    "    ======================\n",
    "    \"\"\"\n",
    "    #  -- Call device --\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -- Setup saving parameters -- \n",
    "    num_epochs = 1\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference/\"\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # Define a path to save the model checkpoints\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference/checkpoints/\"\n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e17b0-fb00-4a3a-bf76-968212b3bed8",
   "metadata": {},
   "source": [
    "Note to self: there are 5184 texts in the training data: train_data_no_ref.csv, and depeding on the batch size, the number of batches pr epoch (number displayed in the tqdm when training the model) will change (e.g. 16 batches will be 324 because 5184/16= 324)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c249fa06-9612-44d3-82ac-c855351484b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 54/54 [17:19<00:00, 19.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.539757353288156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 14/14 [00:42<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4816723210471017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss = 0.5398\n",
      "  Test Loss = 0.4817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "#approx. 18 min to run (1 epoch, 1/6 of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2294e113-0734-48fc-859e-ae0d7cfc8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 01_model_without_reference with a new model (XLM-Roberta)\n",
    "# Current code is working \n",
    "\n",
    "\"\"\"\n",
    "==============================\n",
    "Model Training - No Reference\n",
    "==============================\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Modelling \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification #new ones\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def main_2():\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Set Up -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Parameters -- \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\") # Change model here if desired\n",
    "    batch_size = 16 # Small batch number for processing, can increase if using GPU\n",
    "    num_labels = 12\n",
    "\n",
    "    # Load Data \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_no_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_no_ref.csv\").head(216)\n",
    "\n",
    "    # -- Classes -- \n",
    "    class TextDataset(Dataset):\n",
    "        # initialise the text, labels, the chosen tokenizer (BERT) and set the maximum length to BERT's max (512)\n",
    "        def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        # Return the total number of instances in the dataset (i.e., the number of rows)\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        # Take the text and labels as indexes, and return a dictonary of the tokenised text and its corresponding label \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            encodings = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\"input_ids\": encodings[\"input_ids\"][0], \"attention_mask\": encodings[\"attention_mask\"][0], \"labels\": torch.tensor(label, dtype=torch.float)}\n",
    "\n",
    "    \"\"\"\n",
    "    ==================\n",
    "    ----- Model -----\n",
    "    ==================\n",
    "    \"\"\"\n",
    "    # -- Define labels -- \n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "                                   \n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- Create datasets -- \n",
    "    train_dataset = TextDataset(train_data['rep_text'].tolist(), train_labels, tokenizer, max_length=512)\n",
    "    test_dataset = TextDataset(test_data['rep_text'].tolist(), test_labels, tokenizer, max_length=512)\n",
    "\n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # -- Set the optimizer, loss function and learning rate -- \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-large\", num_labels=num_labels)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # -- Training model -- \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- Evaluate Function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = (logits.sigmoid() > 0.5).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "    \"\"\"\n",
    "    ======================\n",
    "    ----- Test model -----\n",
    "    ======================\n",
    "    \"\"\"\n",
    "    #  -- Call device --\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -- Setup saving parameters -- \n",
    "    num_epochs = 1\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference_new/\"\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # Define a path to save the model checkpoints\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_without_reference_new/checkpoints/\"\n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062ba0ed-374d-4c3f-8a7c-94d66fa9255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████| 54/54 [1:36:59<00:00, 107.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5275223475915415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 14/14 [03:15<00:00, 13.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.49728811212948393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss = 0.5275\n",
      "  Test Loss = 0.4973\n"
     ]
    }
   ],
   "source": [
    "main_2()\n",
    "\n",
    "#approx. 1h 28 min to run (1 epoch, 1/6 of the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08352b5-65c0-4a77-bd19-c06647d5da05",
   "metadata": {},
   "source": [
    "# Model with reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96069172-37fe-43a0-8525-eb663afcdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 02_model_with_ref with the original model\n",
    "# Current code is working \n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "Model Training 2 - Reference Text\n",
    "=================================\n",
    "\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "To include the reference tweet information, there is one modification to this model from the basic 'No Reference Model' (01_model_no_ref.py): \n",
    "1. The reference text is appended to the reply tweet text with a [SEP] token to delineate the two tweets before the classification is made.\n",
    "    - This means the model has more information to base its prediction upon as it can see patterns in the reference text and how this may influence the reply tweet text. \n",
    "\n",
    "Usage:\n",
    "  $ python3 src/02_model_with_ref.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "----- Import Depenendencies -----\n",
    "=================================\n",
    "\"\"\"\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "=========================\n",
    "----- Main Function -----\n",
    "=========================\n",
    "\"\"\"\n",
    "def main_3():\n",
    "    \"\"\"\n",
    "    ==========================\n",
    "    Parameters and Directories \n",
    "    ==========================\n",
    "    \"\"\"\n",
    "    # -- Model parameters -- \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Can change BERT model\n",
    "    batch_size = 16     # small batch size due to computation, increase to 32 if using GPU\n",
    "    num_labels = 12\n",
    "    num_epochs = 1      # Increase if desired, model started to overfit after around 5 epochs\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # -- Directories --\n",
    "    #input_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data\"\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text/\"\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text/checkpoints/\" # path for checkpoints to be saved\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    =======\n",
    "    Classes\n",
    "    =======\n",
    "    \"\"\"\n",
    "    # create a Dataset class for text and reference labels\n",
    "    class TextDatasetWithRefLabels(Dataset):\n",
    "        def __init__(self, ref_texts, ref_labels, texts, labels, tokenizer, max_length):\n",
    "            self.ref_texts = ref_texts\n",
    "            self.ref_labels = ref_labels\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Binarize the reference labels\n",
    "            self.mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "            self.ref_labels = self.mlb.fit_transform(ref_labels)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            ref_text = self.ref_texts[index]\n",
    "            ref_label = self.ref_labels[index]\n",
    "            text = self.texts[index]\n",
    "            label = self.labels[index]\n",
    "\n",
    "            # Tokenize the reply tweet \n",
    "            rep_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # Determine the maximum length of the reference text that can be used \n",
    "            max_ref_length = 511 - len(rep_tokens)\n",
    "\n",
    "            # Tokenize the reference text and truncate if necessary\n",
    "            ref_tokens = self.tokenizer.tokenize(ref_text)\n",
    "            if len(ref_tokens) > max_ref_length:\n",
    "                truncated_ref_tokens = ref_tokens[-max_ref_length:]\n",
    "            else:\n",
    "                truncated_ref_tokens = ref_tokens\n",
    "            \n",
    "            # Combine the reference and reply text tokens with a [SEP] token\n",
    "            combined_tokens = truncated_ref_tokens + [self.tokenizer.sep_token] + rep_tokens \n",
    "\n",
    "            # Convert the combined tokens to a text string\n",
    "            combined_text = self.tokenizer.convert_tokens_to_string(combined_tokens)\n",
    "\n",
    "            # Encode the combined text\n",
    "            encoded_data = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation='only_first',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_data['input_ids']\n",
    "            attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "            # Convert the label data to tensors\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            ref_label_tensor = torch.tensor(ref_label, dtype=torch.float32)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'attention_mask': attention_mask.squeeze(0),\n",
    "                'ref_labels': ref_label_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "\n",
    "    class BertForContextualClassification(nn.Module):\n",
    "        def __init__(self, num_labels):\n",
    "            super(BertForContextualClassification, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.linear = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, ref_input_ids=None, ref_attention_mask=None):\n",
    "            if ref_input_ids is not None:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            else:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            pooled_output = output[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.linear(pooled_output)\n",
    "            return logits\n",
    "\n",
    "    \"\"\"\n",
    "    =============\n",
    "    Preprocessing\n",
    "    =============\n",
    "    \"\"\"\n",
    "    # -- Load Data -- \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_ref.csv\").head(216)\n",
    "\n",
    "    # prepare the train and test labels using MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "\n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- create train and test datasets -- \n",
    "    train_dataset = TextDatasetWithRefLabels(train_data['ref_text'].tolist(),\n",
    "                                            train_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            train_data['rep_text'].tolist(),\n",
    "                                            train_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "\n",
    "    test_dataset = TextDatasetWithRefLabels(test_data['ref_text'].tolist(),\n",
    "                                            test_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            test_data['rep_text'].tolist(),\n",
    "                                            test_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "    \n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \"\"\"\n",
    "    =====\n",
    "    Model\n",
    "    =====\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = BertForContextualClassification(num_labels)\n",
    "\n",
    "    # Define your optimizer, loss function and learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Create training function \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            ref_labels = batch[\"ref_labels\"].to(device)\n",
    "            #print(\"input_ids shape:\", input_ids.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- create evaluation function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).cpu().detach().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "    \n",
    "    # -- Wrap it up into a function -- \n",
    "    # Call device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # Evaluate the training data \n",
    "        #train_loss, true_labels, pred_labels = evaluate(model, train_dataloader, criterion, device)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "        # Activate if kernel struggles to run model \n",
    "        #if epoch < 5:\n",
    "            #del train_loss, true_labels, pred_labels, test_loss, test_true_labels, test_pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad6320a-ff91-4cdb-8682-96a11f46c0ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 54/54 [18:02<00:00, 20.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5449445816101851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 14/14 [02:30<00:00, 10.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4690816445010049\n",
      "  Train Loss = 0.5449\n",
      "  Test Loss = 0.4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main_3()\n",
    "\n",
    "#approx. 19 min to run (1 epoch, 1/6 of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba43483-8205-415d-a55c-53defec1ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 02_model_with_ref with a new model (XLM-Roberta)\n",
    "# Current code is working\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "Model Training 2 - Reference Text\n",
    "=================================\n",
    "\n",
    "This script takes our cleaned twitter data and builds a pipeline to fine-tune a BERT which can classify the tweet into its 3 labels classes. \n",
    "\n",
    "The label classes are as follows: \n",
    "Type (3 labels): Antagonizing, Other political statement, Unclassifiable\n",
    "Tone Hateful (3 labels):  Hateful, Not Hateful, Unclassifiable Hateful \n",
    "Tone Constructive (3 labels): Constructive, Not Constructive, Unclassifiable Constructive\n",
    "\n",
    "To include the reference tweet information, there is one modification to this model from the basic 'No Reference Model' (01_model_no_ref.py): \n",
    "1. The reference text is appended to the reply tweet text with a [SEP] token to delineate the two tweets before the classification is made.\n",
    "    - This means the model has more information to base its prediction upon as it can see patterns in the reference text and how this may influence the reply tweet text. \n",
    "\n",
    "Usage:\n",
    "  $ python3 src/02_model_with_ref.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "=================================\n",
    "----- Import Depenendencies -----\n",
    "=================================\n",
    "\"\"\"\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling \n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "=========================\n",
    "----- Main Function -----\n",
    "=========================\n",
    "\"\"\"\n",
    "\n",
    "def main_4():\n",
    "    \"\"\"\n",
    "    ==========================\n",
    "    Parameters and Directories \n",
    "    ==========================\n",
    "    \"\"\"\n",
    "    # -- Model parameters -- \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\") # Can change BERT model\n",
    "    batch_size = 8     # small batch size due to computation, increase to 32 if using GPU\n",
    "    num_labels = 12\n",
    "    num_epochs = 2      # Increase if desired, model started to overfit after around 5 epochs\n",
    "    epoch_no = 0 \n",
    "\n",
    "    # -- Directories --\n",
    "    #input_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data\"\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_new/\"\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_new/checkpoints/\" # path for checkpoints to be saved\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    =======\n",
    "    Classes\n",
    "    =======\n",
    "    \"\"\"\n",
    "    # create a Dataset class for text and reference labels\n",
    "    class TextDatasetWithRefLabels(Dataset):\n",
    "        def __init__(self, ref_texts, ref_labels, texts, labels, tokenizer, max_length):\n",
    "            self.ref_texts = ref_texts\n",
    "            self.ref_labels = ref_labels\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Binarize the reference labels\n",
    "            self.mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "            self.ref_labels = self.mlb.fit_transform(ref_labels)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            ref_text = self.ref_texts[index]\n",
    "            ref_label = self.ref_labels[index]\n",
    "            text = self.texts[index]\n",
    "            label = self.labels[index]\n",
    "\n",
    "            # Tokenize the reply tweet \n",
    "            rep_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # Determine the maximum length of the reference text that can be used \n",
    "            max_ref_length = 511 - len(rep_tokens)\n",
    "\n",
    "            # Tokenize the reference text and truncate if necessary\n",
    "            ref_tokens = self.tokenizer.tokenize(ref_text)\n",
    "            if len(ref_tokens) > max_ref_length:\n",
    "                truncated_ref_tokens = ref_tokens[-max_ref_length:]\n",
    "            else:\n",
    "                truncated_ref_tokens = ref_tokens\n",
    "            \n",
    "            # Combine the reference and reply text tokens with a [SEP] token\n",
    "            combined_tokens = truncated_ref_tokens + [self.tokenizer.sep_token] + rep_tokens \n",
    "\n",
    "            # Convert the combined tokens to a text string\n",
    "            combined_text = self.tokenizer.convert_tokens_to_string(combined_tokens)\n",
    "\n",
    "            # Encode the combined text\n",
    "            encoded_data = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation='only_first',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_data['input_ids']\n",
    "            attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "            # Convert the label data to tensors\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            ref_label_tensor = torch.tensor(ref_label, dtype=torch.float32)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'attention_mask': attention_mask.squeeze(0),\n",
    "                'ref_labels': ref_label_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "\n",
    "    class RobertaForContextualClassification(nn.Module):\n",
    "        def __init__(self, num_labels):\n",
    "            super(RobertaForContextualClassification, self).__init__() \n",
    "            self.bert = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.linear = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, ref_input_ids=None, ref_attention_mask=None):\n",
    "            if ref_input_ids is not None:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            else:\n",
    "                output = self.bert(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None\n",
    "                )\n",
    "            pooled_output = output[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "            logits = self.linear(pooled_output)\n",
    "            return logits\n",
    "\n",
    "    \"\"\"\n",
    "    =============\n",
    "    Preprocessing\n",
    "    =============\n",
    "    \"\"\"\n",
    "    # -- Load Data -- \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_ref.csv\").head(288) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_ref.csv\").head(72)\n",
    "\n",
    "\n",
    "    # prepare the train and test labels using MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "\n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # -- create train and test datasets -- \n",
    "    train_dataset = TextDatasetWithRefLabels(train_data['ref_text'].tolist(),\n",
    "                                            train_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            train_data['rep_text'].tolist(),\n",
    "                                            train_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "\n",
    "    test_dataset = TextDatasetWithRefLabels(test_data['ref_text'].tolist(),\n",
    "                                            test_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                            test_data['rep_text'].tolist(),\n",
    "                                            test_labels,\n",
    "                                            tokenizer,\n",
    "                                            max_length=512)\n",
    "    \n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \"\"\"\n",
    "    =====\n",
    "    Model\n",
    "    =====\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = RobertaForContextualClassification(num_labels) \n",
    "\n",
    "    # Define your optimizer, loss function and learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Create training function \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            ref_labels = batch[\"ref_labels\"].to(device)\n",
    "            #print(\"input_ids shape:\", input_ids.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- create evaluation function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).cpu().detach().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "    \n",
    "    # -- Wrap it up into a function -- \n",
    "    # Call device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # Evaluate the training data \n",
    "        train_loss, true_labels, pred_labels = evaluate(model, train_dataloader, criterion, device)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "        # Activate if kernel struggles to run model \n",
    "        if epoch < 5:\n",
    "            del train_loss, true_labels, pred_labels, test_loss, test_true_labels, test_pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37b531-3f47-4a66-ae7a-d57cc4330a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idahelenedencker/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███████████                      | 12/36 [11:57<24:39, 61.66s/it]"
     ]
    }
   ],
   "source": [
    "main_4() \n",
    "\n",
    "#aprrox. 2h 18m to run (kernel died when evaluating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed47ea4-1dfb-43cf-8445-e54ffb836fd0",
   "metadata": {},
   "source": [
    "# Model with reference and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e892ea1-1d24-4567-a20b-99eba2b08645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 03_model_with_ref_and_labels with the original model\n",
    "# Current code is working \n",
    "\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling \n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def main_5():\n",
    "    \"\"\"\n",
    "    ==========================\n",
    "    Parameters and Directories \n",
    "    ==========================\n",
    "    \"\"\"\n",
    "    # Argparse parameters \n",
    "    model_name = 'bert-base-uncased'\n",
    "    num_epochs = 1\n",
    "\n",
    "    # Model parameters \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    batch_size = 16    # Increase to 32 if using on GPU for faster training\n",
    "    num_ref_labels = 12\n",
    "    num_labels = 12\n",
    "\n",
    "    # Directories \n",
    "    #input_dir = \"./data/\"\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_and_labels/\"\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_and_labels/checkpoints/\" # path for checkpoints to be saved\n",
    "    \n",
    "    \"\"\"\n",
    "    =======\n",
    "    Classes\n",
    "    =======\n",
    "    \"\"\"\n",
    "    # -- Class for creating dataset with referemce text and labels -- \n",
    "    class TextDatasetWithRefLabels(Dataset):\n",
    "        def __init__(self, ref_texts, ref_labels, texts, labels, tokenizer, max_length):\n",
    "            self.ref_texts = ref_texts\n",
    "            self.ref_labels = ref_labels\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Binarize the reference labels\n",
    "            self.mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "            self.ref_labels = self.mlb.fit_transform(ref_labels)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            ref_text = self.ref_texts[index]\n",
    "            ref_label = self.ref_labels[index]\n",
    "            text = self.texts[index]\n",
    "            label = self.labels[index]\n",
    "\n",
    "            # Tokenize the reply tweet \n",
    "            rep_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # Determine the maximum length of the reference text that can be used \n",
    "            max_ref_length = 511 - len(rep_tokens)\n",
    "\n",
    "            # Tokenize the reference text and truncate if necessary\n",
    "            ref_tokens = self.tokenizer.tokenize(ref_text)\n",
    "            if len(ref_tokens) > max_ref_length:\n",
    "                truncated_ref_tokens = ref_tokens[-max_ref_length:]\n",
    "            else:\n",
    "                truncated_ref_tokens = ref_tokens\n",
    "                \n",
    "            # Combine the reference and reply text tokens with a [SEP] token\n",
    "            combined_tokens = truncated_ref_tokens + [self.tokenizer.sep_token] + rep_tokens \n",
    "\n",
    "            # Convert the combined tokens to a text string\n",
    "            combined_text = self.tokenizer.convert_tokens_to_string(combined_tokens)\n",
    "\n",
    "            # Encode the combined text\n",
    "            encoded_data = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation='only_first',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_data['input_ids']\n",
    "            attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "            # Convert the label data to tensors\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            ref_label_tensor = torch.tensor(ref_label, dtype=torch.float32)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'attention_mask': attention_mask.squeeze(0),\n",
    "                'ref_labels': ref_label_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "    \n",
    "    # -- class for modelling with reference text and labels -- \n",
    "    class BertForContextualClassification(nn.Module):\n",
    "        def __init__(self, num_labels, num_ref_labels, model_name):\n",
    "            super(BertForContextualClassification, self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.linear_ref_labels = nn.Linear(num_ref_labels, num_ref_labels) # Additional layer for reference labels\n",
    "            self.linear_combined = nn.Linear(self.bert.config.hidden_size + num_ref_labels, num_labels) # Combining BERT output and reference labels\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, ref_labels):\n",
    "            # Obtain BERT's output for the combined text\n",
    "            output = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            pooled_output = output[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            # Pass the reference labels through a linear layer\n",
    "            ref_labels_output = self.linear_ref_labels(ref_labels)\n",
    "\n",
    "            # Concatenate the BERT pooled output with the processed reference labels\n",
    "            combined_output = torch.cat((pooled_output, ref_labels_output), dim=1)\n",
    "\n",
    "            # Pass the combined output through the final linear layer\n",
    "            logits = self.linear_combined(combined_output)\n",
    "\n",
    "            return logits\n",
    "\n",
    "    \"\"\"\n",
    "    =============\n",
    "    Preprocessing\n",
    "    =============\n",
    "    \"\"\"\n",
    "    # -- Load Data -- \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_ref.csv\").head(864) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_ref.csv\").head(216)\n",
    "\n",
    "    #Define the labels to be binarised and transform\n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # create datasets\n",
    "    train_dataset = TextDatasetWithRefLabels(ref_texts=train_data['ref_text'].tolist(),\n",
    "                                          ref_labels=train_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                          texts=train_data['rep_text'].tolist(),\n",
    "                                          labels=train_labels,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          max_length=512)\n",
    "    \n",
    "    test_dataset = TextDatasetWithRefLabels(ref_texts=test_data['ref_text'].tolist(),\n",
    "                                         ref_labels=test_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                         texts=test_data['rep_text'].tolist(),\n",
    "                                         labels=test_labels,\n",
    "                                         tokenizer=tokenizer,\n",
    "                                         max_length=512)\n",
    "\n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \"\"\"\n",
    "    =====\n",
    "    Model\n",
    "    =====\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = BertForContextualClassification(num_labels, num_ref_labels, model_name)\n",
    "\n",
    "    # Define your optimizer, loss function and learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Create training function \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            ref_labels = batch[\"ref_labels\"].to(device) # Extract reference labels from the batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask, ref_labels=ref_labels) # Include reference labels when calling the model\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- create evaluation function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            ref_labels = batch[\"ref_labels\"].to(device) # Extract reference labels from the batch\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask, ref_labels=ref_labels) # Include reference labels when calling the model\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).cpu().detach().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "    # Call device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -- Wrap it up into a function -- \n",
    "    epoch_no = 0 \n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b88461-9d06-43e5-a706-9724b1022e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 54/54 [18:25<00:00, 20.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5775867501894633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 14/14 [02:16<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.47699148314339773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss = 0.5776\n",
      "  Test Loss = 0.4770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "main_5()\n",
    "\n",
    "#approx. 19 min to run (1 epoch, 1/6 of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf6304f-5655-49cb-af01-f274cb402418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the 03_model_with_ref_and_labels with a new model (XLM-Roberta)\n",
    "\n",
    "\n",
    "# Data processing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling \n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def main_6():\n",
    "    \"\"\"\n",
    "    ==========================\n",
    "    Parameters and Directories \n",
    "    ==========================\n",
    "    \"\"\"\n",
    "    # Argparse parameters \n",
    "    model_name = 'xlm-roberta-large'\n",
    "    num_epochs = 1\n",
    "\n",
    "    # Model parameters \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "    batch_size = 16    # Increase to 32 if using on GPU for faster training\n",
    "    num_ref_labels = 12\n",
    "    num_labels = 12\n",
    "\n",
    "    # Directories \n",
    "    #input_dir = \"./data/\"\n",
    "    output_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_and_labels_new/\"\n",
    "    checkpoint_dir = \"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/output/model_with_reference_text_and_labels_new/checkpoints/\" # path for checkpoints to be saved\n",
    "    \n",
    "    \"\"\"\n",
    "    =======\n",
    "    Classes\n",
    "    =======\n",
    "    \"\"\"\n",
    "    # -- Class for creating dataset with referemce text and labels -- \n",
    "    class TextDatasetWithRefLabels(Dataset):\n",
    "        def __init__(self, ref_texts, ref_labels, texts, labels, tokenizer, max_length):\n",
    "            self.ref_texts = ref_texts\n",
    "            self.ref_labels = ref_labels\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # Binarize the reference labels\n",
    "            self.mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "            self.ref_labels = self.mlb.fit_transform(ref_labels)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            ref_text = self.ref_texts[index]\n",
    "            ref_label = self.ref_labels[index]\n",
    "            text = self.texts[index]\n",
    "            label = self.labels[index]\n",
    "\n",
    "            # Tokenize the reply tweet \n",
    "            rep_tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # Determine the maximum length of the reference text that can be used \n",
    "            max_ref_length = 511 - len(rep_tokens)\n",
    "\n",
    "            # Tokenize the reference text and truncate if necessary\n",
    "            ref_tokens = self.tokenizer.tokenize(ref_text)\n",
    "            if len(ref_tokens) > max_ref_length:\n",
    "                truncated_ref_tokens = ref_tokens[-max_ref_length:]\n",
    "            else:\n",
    "                truncated_ref_tokens = ref_tokens\n",
    "                \n",
    "            # Combine the reference and reply text tokens with a [SEP] token\n",
    "            combined_tokens = truncated_ref_tokens + [self.tokenizer.sep_token] + rep_tokens \n",
    "\n",
    "            # Convert the combined tokens to a text string\n",
    "            combined_text = self.tokenizer.convert_tokens_to_string(combined_tokens)\n",
    "\n",
    "            # Encode the combined text\n",
    "            encoded_data = self.tokenizer.encode_plus(\n",
    "                combined_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation='only_first',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoded_data['input_ids']\n",
    "            attention_mask = encoded_data['attention_mask']\n",
    "\n",
    "            # Convert the label data to tensors\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            ref_label_tensor = torch.tensor(ref_label, dtype=torch.float32)\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'attention_mask': attention_mask.squeeze(0),\n",
    "                'ref_labels': ref_label_tensor,\n",
    "                'labels': label_tensor\n",
    "            }\n",
    "    \n",
    "    # -- class for modelling with reference text and labels -- \n",
    "    class RobertaForContextualClassification(nn.Module):\n",
    "        def __init__(self, num_labels, num_ref_labels, model_name):\n",
    "            super(RobertaForContextualClassification, self).__init__()\n",
    "            self.bert = XLMRobertaModel.from_pretrained(model_name)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.linear_ref_labels = nn.Linear(num_ref_labels, num_ref_labels) # Additional layer for reference labels\n",
    "            self.linear_combined = nn.Linear(self.bert.config.hidden_size + num_ref_labels, num_labels) # Combining BERT output and reference labels\n",
    "\n",
    "        def forward(self, input_ids, attention_mask, ref_labels):\n",
    "            # Obtain BERT's output for the combined text\n",
    "            output = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            pooled_output = output[1]\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            # Pass the reference labels through a linear layer\n",
    "            ref_labels_output = self.linear_ref_labels(ref_labels)\n",
    "\n",
    "            # Concatenate the BERT pooled output with the processed reference labels\n",
    "            combined_output = torch.cat((pooled_output, ref_labels_output), dim=1)\n",
    "\n",
    "            # Pass the combined output through the final linear layer\n",
    "            logits = self.linear_combined(combined_output)\n",
    "\n",
    "            return logits\n",
    "\n",
    "    \"\"\"\n",
    "    =============\n",
    "    Preprocessing\n",
    "    =============\n",
    "    \"\"\"\n",
    "    # -- Load Data -- \n",
    "    train_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/train/train_data_ref.csv\").head(288) #smaller sample for testing\n",
    "    test_data = pd.read_csv(\"/Users/idahelenedencker/Desktop/STANDBY_Ida/Retraining and finetuning counterspeech classifier/Data/test/test_data_ref.csv\").head(72)\n",
    "\n",
    "    #Define the labels to be binarised and transform\n",
    "    mlb = MultiLabelBinarizer(classes=['Antagonizing', 'Other political statement', 'Unclassifiable', 'Hateful', 'Not Hateful', 'Unclassifiable Hateful', 'Constructive', 'Not Constructive','Unclassifiable Constructive', 'Agree', 'Disagree', 'Unclear'])\n",
    "    train_labels = mlb.fit_transform(train_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "    test_labels = mlb.transform(test_data[['rep_type', 'rep_hateful', 'rep_constructive', 'rep_agree']].values)\n",
    "\n",
    "    # create datasets\n",
    "    train_dataset = TextDatasetWithRefLabels(ref_texts=train_data['ref_text'].tolist(),\n",
    "                                          ref_labels=train_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                          texts=train_data['rep_text'].tolist(),\n",
    "                                          labels=train_labels,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          max_length=512)\n",
    "    \n",
    "    test_dataset = TextDatasetWithRefLabels(ref_texts=test_data['ref_text'].tolist(),\n",
    "                                         ref_labels=test_data[['ref_type', 'ref_hateful', 'ref_constructive']].values,\n",
    "                                         texts=test_data['rep_text'].tolist(),\n",
    "                                         labels=test_labels,\n",
    "                                         tokenizer=tokenizer,\n",
    "                                         max_length=512)\n",
    "\n",
    "    # create train and test dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \"\"\"\n",
    "    =====\n",
    "    Model\n",
    "    =====\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    model = RobertaForContextualClassification(num_labels, num_ref_labels, model_name)\n",
    "\n",
    "    # Define your optimizer, loss function and learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Create training function \n",
    "    def train(model, dataloader, criterion, optimizer, device):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            ref_labels = batch[\"ref_labels\"].to(device) # Extract reference labels from the batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask, ref_labels=ref_labels) # Include reference labels when calling the model\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss\n",
    "    \n",
    "    # -- create evaluation function -- \n",
    "    def evaluate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            ref_labels = batch[\"ref_labels\"].to(device) # Extract reference labels from the batch\n",
    "\n",
    "            logits = model(input_ids, attention_mask=attention_mask, ref_labels=ref_labels) # Include reference labels when calling the model\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (logits.sigmoid() > 0.5).cpu().detach().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        return avg_loss, np.array(all_labels), np.array(all_preds)\n",
    "    \n",
    "    # Call device \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # -- Wrap it up into a function -- \n",
    "    epoch_no = 0 \n",
    "\n",
    "    # Define the model checkpoint file format (e.g., \"model_epoch_{epoch}.pt\")\n",
    "    checkpoint_filename = \"model_epoch_{epoch}.pt\"\n",
    "\n",
    "    # Define the label classes\n",
    "    label_classes = {\n",
    "        'rep_type': ['Antagonizing', 'Other political statement', 'Unclassifiable'],\n",
    "        'rep_constructive': ['Constructive', 'Not Constructive', 'Unclassifiable Constructive'],\n",
    "        'rep_hateful': ['Hateful', 'Not Hateful', 'Unclassifiable Hateful'],\n",
    "        'rep_agree': ['Agree', 'Disagree', 'Unclear']\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Free up memory by deleting variables\n",
    "        epoch_no = epoch_no + 1\n",
    "        print(f\"Epoch {epoch_no}:\")\n",
    "        # -- Train model -- \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = checkpoint_dir + checkpoint_filename.format(epoch=epoch_no)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # -- Test model -- \n",
    "        test_loss, test_true_labels, test_pred_labels = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # Convert output from one hot encoding \n",
    "        true_labels = np.array(test_true_labels)\n",
    "        pred_labels = np.array(test_pred_labels)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        for class_name, labels in label_classes.items():\n",
    "            # Get indices corresponding to labels in this class\n",
    "            indices = [i for i, label in enumerate(mlb.classes_) if label in labels]\n",
    "            \n",
    "            # Extract true and predicted values for this class\n",
    "            class_true = np.argmax(true_labels[:, indices], axis=1)\n",
    "            class_pred = np.argmax(pred_labels[:, indices], axis=1)\n",
    "            \n",
    "            # Compute the confusion matrix\n",
    "            matrix = confusion_matrix(class_true, class_pred)\n",
    "\n",
    "            # Save or visualize the confusion matrix\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.title(f'Confusion Matrix for {class_name}')\n",
    "            plt.savefig(f'{output_dir}confusion_matrix_{class_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "        # Generate report \n",
    "        report = classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_, output_dict=True)\n",
    "        report_name = output_dir + \"classification_report_\" + str(epoch_no) + \".txt\"\n",
    "\n",
    "        with open(report_name, \"w\") as f:\n",
    "            f.write(classification_report(test_true_labels, test_pred_labels, target_names=mlb.classes_))\n",
    "        \n",
    "        # Print results \n",
    "        #print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss = {train_loss:.4f}\")\n",
    "        print(f\"  Test Loss = {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93d54b8-b515-43d3-9235-637c4d82570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idahelenedencker/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|███████▎                         | 4/18 [07:21<25:44, 110.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_6\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#approx. 1h 32 min to run (1 epoch, 1/6 of the data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#(kernel died when evaluating)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 264\u001b[0m, in \u001b[0;36mmain_6\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# -- Train model -- \u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Save model checkpoint\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 204\u001b[0m, in \u001b[0;36mmain_6.<locals>.train\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    201\u001b[0m ref_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mref_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Extract reference labels from the batch\u001b[39;00m\n\u001b[1;32m    203\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 204\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref_labels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Include reference labels when calling the model\u001b[39;00m\n\u001b[1;32m    205\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m    206\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 128\u001b[0m, in \u001b[0;36mmain_6.<locals>.RobertaForContextualClassification.forward\u001b[0;34m(self, input_ids, attention_mask, ref_labels)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, ref_labels):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Obtain BERT's output for the combined text\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    133\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:834\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    825\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    827\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    828\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    829\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    833\u001b[0m )\n\u001b[0;32m--> 834\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    847\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:522\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    511\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    512\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    513\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         output_attentions,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 522\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:411\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    401\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:338\u001b[0m, in \u001b[0;36mXLMRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    330\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 338\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:267\u001b[0m, in \u001b[0;36mXLMRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 267\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    270\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_6()\n",
    "\n",
    "#approx. 1h 32 min to run (1 epoch, 1/6 of the data)\n",
    "#(kernel died when evaluating)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
