{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7066e7a-4020-41b2-9214-9a2bd8be04c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas torch transformers numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f33f5-9beb-491b-9402-934ea641ce48",
   "metadata": {},
   "source": [
    "# Test model without reference (XLM-roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c44bfd8a-0260-490c-ad29-68e236479e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_622/3182014777.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/work/IdaHeleneDencker#2808/Redoing_counterspeech_classifier/output/model_without_reference/checkpoints/model_epoch_3.pt\"))\n",
      "Applying Model: 100%|██████████| 7/7 [00:01<00:00,  4.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_class_hateful</th>\n",
       "      <th>predicted_class_constructive</th>\n",
       "      <th>predicted_class_agree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>Unclassifiable Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate you</td>\n",
       "      <td>Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Acosta You are more than right Jim.  Such a s...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i very much agree, i think that is a great point</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i think you should watch your tone, it can be ...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making fun of a serious health condition is no...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text predicted_class_hateful  \\\n",
       "0                                               Test  Unclassifiable Hateful   \n",
       "1                                         I hate you                 Hateful   \n",
       "2                                         I love you             Not Hateful   \n",
       "3  @Acosta You are more than right Jim.  Such a s...             Not Hateful   \n",
       "4   i very much agree, i think that is a great point             Not Hateful   \n",
       "5  i think you should watch your tone, it can be ...             Not Hateful   \n",
       "6  Making fun of a serious health condition is no...             Not Hateful   \n",
       "\n",
       "  predicted_class_constructive predicted_class_agree  \n",
       "0             Not Constructive               Unclear  \n",
       "1             Not Constructive              Disagree  \n",
       "2                 Constructive                 Agree  \n",
       "3             Not Constructive                 Agree  \n",
       "4             Not Constructive                 Agree  \n",
       "5                 Constructive              Disagree  \n",
       "6                 Constructive              Disagree  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code works\n",
    "# I think it does a decent job in classifiying!\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained model architecture\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-large\",  num_labels=9)  \n",
    "# Load your custom saved weights\n",
    "model.load_state_dict(torch.load(\"/work/IdaHeleneDencker#2808/Redoing_counterspeech_classifier/output/model_without_reference/checkpoints/model_epoch_3.pt\"))\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simple list of texts to classify\n",
    "data = ['Test', \n",
    "        'I hate you', \n",
    "        'I love you',\n",
    "        '@Acosta You are more than right Jim.  Such a shame that there are so many ignorant fools in this country.', \n",
    "        'i very much agree, i think that is a great point', \n",
    "        'i think you should watch your tone, it can be hurtful to other peple',\n",
    "        'Making fun of a serious health condition is not the way to get your point across.']\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Make sure to disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Forward pass through the model\n",
    "\n",
    "    # Get predicted class\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Split the logits for each task\n",
    "    logits_hateful = logits[:, :3]          \n",
    "    logits_constructive = logits[:, 3:6] \n",
    "    logits_agree = logits[:, 6:]\n",
    "    #print(\"logits agree\",logits_agree)\n",
    "    \n",
    "    preds_hateful = torch.argmax(logits_hateful, dim=1).item()\n",
    "    preds_constructive = torch.argmax(logits_constructive, dim=1).item()\n",
    "    preds_agree = torch.argmax(logits_agree, dim=1).item()\n",
    "    #print(\"preds agree\",preds_agree)\n",
    "    \n",
    "    return preds_hateful, preds_constructive, preds_agree\n",
    "\n",
    "\n",
    "\n",
    "# List to store the predicted classes\n",
    "predicted_class_hateful = []\n",
    "predicted_class_constructive = []\n",
    "predicted_class_agree = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each text for prediction\n",
    "for text in tqdm(df['text'], desc=\"Applying Model\"):\n",
    "    result = predict(text)\n",
    "    #print(result)\n",
    "    # Append to the list\n",
    "    predicted_class_hateful.append(result[0])\n",
    "    predicted_class_constructive.append(result[1])\n",
    "    predicted_class_agree.append(result[2])\n",
    "\n",
    "\n",
    "# Add the predicted classes as a new column in the DataFrame\n",
    "df['predicted_class_hateful'] = predicted_class_hateful\n",
    "df['predicted_class_constructive'] = predicted_class_constructive\n",
    "df['predicted_class_agree'] = predicted_class_agree\n",
    "\n",
    "\n",
    "label_mappings = {\n",
    "    'predicted_class_hateful': {0: 'Hateful', 1: 'Not Hateful', 2:'Unclassifiable Hateful'},\n",
    "    'predicted_class_constructive': {0: 'Not Constructive', 1: 'Constructive',2: 'Unclassifiable Constructive'},\n",
    "    'predicted_class_agree': {0: 'Agree', 1: 'Disagree',2:  'Unclear'}}\n",
    "for column, mapping in label_mappings.items():\n",
    "    df[column] = df[column].map(mapping)\n",
    "\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a366d76-f9b7-497b-b527-2d58f5c20a02",
   "metadata": {},
   "source": [
    "# Test model with reference (XLM-roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89bba729-c714-4594-88d7-6785f3d39e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_928/2234694044.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/work/IdaHeleneDencker#2808/Redoing_counterspeech_classifier/output/model_with_reference/checkpoints/model_epoch_4.pt\"))\n",
      "Applying Model: 100%|██████████| 7/7 [00:01<00:00,  5.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_class_hateful</th>\n",
       "      <th>predicted_class_constructive</th>\n",
       "      <th>predicted_class_agree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate you</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Acosta You are more than right Jim.  Such a s...</td>\n",
       "      <td>Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i very much agree, i think that is a great point</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i think you should watch your tone, it can be ...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making fun of a serious health condition is no...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text predicted_class_hateful  \\\n",
       "0                                               Test             Not Hateful   \n",
       "1                                         I hate you             Not Hateful   \n",
       "2                                         I love you             Not Hateful   \n",
       "3  @Acosta You are more than right Jim.  Such a s...                 Hateful   \n",
       "4   i very much agree, i think that is a great point             Not Hateful   \n",
       "5  i think you should watch your tone, it can be ...             Not Hateful   \n",
       "6  Making fun of a serious health condition is no...             Not Hateful   \n",
       "\n",
       "  predicted_class_constructive predicted_class_agree  \n",
       "0             Not Constructive               Unclear  \n",
       "1             Not Constructive              Disagree  \n",
       "2             Not Constructive                 Agree  \n",
       "3             Not Constructive                 Agree  \n",
       "4             Not Constructive                 Agree  \n",
       "5             Not Constructive              Disagree  \n",
       "6                 Constructive              Disagree  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use if model apllied is using the default roberta model\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, XLMRobertaModel\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained model architecture\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-large\",  num_labels=9) \n",
    "\n",
    "\n",
    "# Load your custom saved weights\n",
    "model.load_state_dict(torch.load(\"/work/IdaHeleneDencker#2808/Redoing_counterspeech_classifier/output/model_with_reference/checkpoints/model_epoch_4.pt\"))\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Simple list of texts to classify\n",
    "data = ['Test', \n",
    "        'I hate you', \n",
    "        'I love you',\n",
    "        '@Acosta You are more than right Jim.  Such a shame that there are so many ignorant fools in this country.', \n",
    "        'i very much agree, i think that is a great point', \n",
    "        'i think you should watch your tone, it can be hurtful to other peple',\n",
    "        'Making fun of a serious health condition is not the way to get your point across.']\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Make sure to disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Forward pass through the model \n",
    "        \n",
    "    # Get predicted class\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Split the logits for each task\n",
    "    logits_hateful = logits[:, :3]          \n",
    "    logits_constructive = logits[:, 3:6] \n",
    "    logits_agree = logits[:, 6:]\n",
    "    #print(\"logits agree\",logits_agree)\n",
    "    \n",
    "    preds_hateful = torch.argmax(logits_hateful, dim=1).item()\n",
    "    preds_constructive = torch.argmax(logits_constructive, dim=1).item()\n",
    "    preds_agree = torch.argmax(logits_agree, dim=1).item()\n",
    "    #print(\"preds agree\",preds_agree)\n",
    "    \n",
    "    return preds_hateful, preds_constructive, preds_agree\n",
    "\n",
    "\n",
    "\n",
    "# List to store the predicted classes\n",
    "predicted_class_hateful = []\n",
    "predicted_class_constructive = []\n",
    "predicted_class_agree = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each text for prediction\n",
    "for text in tqdm(df['text'], desc=\"Applying Model\"):\n",
    "    result = predict(text)\n",
    "    #print(result)\n",
    "    # Append to the list\n",
    "    predicted_class_hateful.append(result[0])\n",
    "    predicted_class_constructive.append(result[1])\n",
    "    predicted_class_agree.append(result[2])\n",
    "\n",
    "\n",
    "# Add the predicted classes as a new column in the DataFrame\n",
    "df['predicted_class_hateful'] = predicted_class_hateful\n",
    "df['predicted_class_constructive'] = predicted_class_constructive\n",
    "df['predicted_class_agree'] = predicted_class_agree\n",
    "\n",
    "\n",
    "label_mappings = {\n",
    "    'predicted_class_hateful': {0: 'Hateful', 1: 'Not Hateful', 2:'Unclassifiable Hateful'},\n",
    "    'predicted_class_constructive': {0: 'Not Constructive', 1: 'Constructive',2: 'Unclassifiable Constructive'},\n",
    "    'predicted_class_agree': {0: 'Agree', 1: 'Disagree',2:  'Unclear'}}\n",
    "for column, mapping in label_mappings.items():\n",
    "    df[column] = df[column].map(mapping)\n",
    "\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffae0094-0c23-4b49-9d61-dd4b63c68a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_928/3529757259.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/work/IdaHeleneDencker#2808/Redoing_counterspeech_classifier/output/model_with_reference/checkpoints/model_epoch_2.pt\"))\n",
      "Applying Model: 100%|██████████| 7/7 [00:01<00:00,  3.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_class_hateful</th>\n",
       "      <th>predicted_class_constructive</th>\n",
       "      <th>predicted_class_agree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>Unclassifiable Hateful</td>\n",
       "      <td>Unclassifiable Constructive</td>\n",
       "      <td>Unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate you</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Acosta You are more than right Jim.  Such a s...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i very much agree, i think that is a great point</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Not Constructive</td>\n",
       "      <td>Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i think you should watch your tone, it can be ...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making fun of a serious health condition is no...</td>\n",
       "      <td>Not Hateful</td>\n",
       "      <td>Constructive</td>\n",
       "      <td>Disagree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text predicted_class_hateful  \\\n",
       "0                                               Test  Unclassifiable Hateful   \n",
       "1                                         I hate you             Not Hateful   \n",
       "2                                         I love you             Not Hateful   \n",
       "3  @Acosta You are more than right Jim.  Such a s...             Not Hateful   \n",
       "4   i very much agree, i think that is a great point             Not Hateful   \n",
       "5  i think you should watch your tone, it can be ...             Not Hateful   \n",
       "6  Making fun of a serious health condition is no...             Not Hateful   \n",
       "\n",
       "  predicted_class_constructive predicted_class_agree  \n",
       "0  Unclassifiable Constructive               Unclear  \n",
       "1             Not Constructive               Unclear  \n",
       "2                 Constructive                 Agree  \n",
       "3             Not Constructive                 Agree  \n",
       "4             Not Constructive                 Agree  \n",
       "5                 Constructive              Disagree  \n",
       "6                 Constructive              Disagree  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use if model apllied is using the custom class roberta model\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, XLMRobertaModel\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "\n",
    "# Used if the model to be applied is using the customized model \n",
    "class RobertaForContextualClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(RobertaForContextualClassification, self).__init__() \n",
    "        self.roberta = XLMRobertaModel.from_pretrained('xlm-roberta-large') # Initializes the XLM-RoBERTa model from Hugging Face's transformers library\n",
    "        self.dropout = nn.Dropout(0.1) # Adds a dropout layer (regularization technique) to to prevent overfitting = 10% of the neurons are randomly deactivated during training\n",
    "        self.linear = nn.Linear(self.roberta.config.hidden_size, num_labels) # Adds a linear layer that maps hidden size (the output size of the model's hidden representations) to the number of classification labels (num_labels). This linear layer is responsible for producing the logits (predictions) for each class.\n",
    "\n",
    "    def forward(self, input_ids, attention_mask): \n",
    "        # Processes the input IDs and attention mask through the model and returns an output\n",
    "        output = self.roberta(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=None) \n",
    "\n",
    "        pooled_output = output[1] # Takes second element of output, the pooled output which is the hidden state of the [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output) # A dropout layer is applied to reduce overfitting\n",
    "        logits = self.linear(pooled_output) # The pooled output is then passed through the linear layer, which produces logits for each class.\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained model architecture\n",
    "#model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-large\",  num_labels=9) #change if using customized model, to:\n",
    "model = RobertaForContextualClassification(num_labels=9) \n",
    "\n",
    "\n",
    "\n",
    "# Load your custom saved weights\n",
    "model.load_state_dict(torch.load(\"/work/IdaHeleneDencker#2808/Redoing_counterspeech_classifier/output/model_with_reference/checkpoints/model_epoch_2.pt\"))\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simple list of texts to classify\n",
    "data = ['Test', \n",
    "        'I hate you', \n",
    "        'I love you',\n",
    "        '@Acosta You are more than right Jim.  Such a shame that there are so many ignorant fools in this country.', \n",
    "        'i very much agree, i think that is a great point', \n",
    "        'i think you should watch your tone, it can be hurtful to other peple',\n",
    "        'Making fun of a serious health condition is not the way to get your point across.']\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    input_ids = inputs['input_ids']  # Extract input IDs\n",
    "    attention_mask = inputs['attention_mask']  # Extract attention mask\n",
    "    \n",
    "    # Make sure to disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask) \n",
    "\n",
    "    # Split the logits for each task\n",
    "    logits_hateful = logits[:, :3]          \n",
    "    logits_constructive = logits[:, 3:6] \n",
    "    logits_agree = logits[:, 6:]\n",
    "    \n",
    "    preds_hateful = torch.argmax(logits_hateful, dim=1).item()\n",
    "    preds_constructive = torch.argmax(logits_constructive, dim=1).item()\n",
    "    preds_agree = torch.argmax(logits_agree, dim=1).item()\n",
    "    \n",
    "    return preds_hateful, preds_constructive, preds_agree\n",
    "\n",
    "\n",
    "\n",
    "# List to store the predicted classes\n",
    "predicted_class_hateful = []\n",
    "predicted_class_constructive = []\n",
    "predicted_class_agree = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each text for prediction\n",
    "for text in tqdm(df['text'], desc=\"Applying Model\"):\n",
    "    result = predict(text)\n",
    "    # Append to the list\n",
    "    predicted_class_hateful.append(result[0])\n",
    "    predicted_class_constructive.append(result[1])\n",
    "    predicted_class_agree.append(result[2])\n",
    "\n",
    "\n",
    "# Add the predicted classes as a new column in the DataFrame\n",
    "df['predicted_class_hateful'] = predicted_class_hateful\n",
    "df['predicted_class_constructive'] = predicted_class_constructive\n",
    "df['predicted_class_agree'] = predicted_class_agree\n",
    "\n",
    "\n",
    "label_mappings = {\n",
    "    'predicted_class_hateful': {0: 'Hateful', 1: 'Not Hateful', 2:'Unclassifiable Hateful'},\n",
    "    'predicted_class_constructive': {0: 'Not Constructive', 1: 'Constructive',2: 'Unclassifiable Constructive'},\n",
    "    'predicted_class_agree': {0: 'Agree', 1: 'Disagree',2:  'Unclear'}}\n",
    "for column, mapping in label_mappings.items():\n",
    "    df[column] = df[column].map(mapping)\n",
    "\n",
    "\n",
    "# Display the DataFrame with predictions\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
